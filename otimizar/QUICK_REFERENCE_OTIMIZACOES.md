# ğŸš€ Quick Reference - OtimizaÃ§Ãµes zmatrix.cpp

## ğŸ“‹ Tabela de MÃ©todos e OtimizaÃ§Ãµes

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        MATRIZ COMPLETA DE OTIMIZAÃ‡Ã•ES                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ MÃ©todo              â”‚ SIMD â”‚ OpenMP â”‚ BLAS â”‚ CUDA â”‚ GPU_Device â”‚ Restrict â”‚ Status â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                        OPERAÃ‡Ã•ES ARITMÃ‰TICAS BÃSICAS                                â•‘
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â•‘ add()               â”‚  âœ…  â”‚   âœ…   â”‚  âŒ  â”‚  âœ…  â”‚     âŒ     â”‚    âœ…    â”‚ âœ… OK  â•‘
â•‘ subtract()          â”‚  âœ…  â”‚   âœ…   â”‚  âŒ  â”‚  âœ…  â”‚     âŒ     â”‚    âœ…    â”‚ âœ… OK  â•‘
â•‘ mul() (elem-wise)   â”‚  âœ…  â”‚   âœ…   â”‚  âŒ  â”‚  âœ…  â”‚     âŒ     â”‚    âœ…    â”‚ âœ… OK  â•‘
â•‘ divide()            â”‚  âŒ  â”‚   âœ…   â”‚  âŒ  â”‚  âœ…  â”‚     âŒ     â”‚    âœ…    â”‚ âš ï¸ GAP â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                           OPERAÃ‡Ã•ES ESCALARES                                      â•‘
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â•‘ scalar_add()        â”‚  âœ…  â”‚   âœ…   â”‚  âŒ  â”‚  âœ…  â”‚     âœ…     â”‚    âŒ    â”‚ âœ… OK  â•‘
â•‘ scalar_subtract()   â”‚  âœ…  â”‚   âœ…   â”‚  âŒ  â”‚  âœ…  â”‚     âœ…     â”‚    âŒ    â”‚ âœ… OK  â•‘
â•‘ scalar_divide()     â”‚  âœ…  â”‚   âœ…   â”‚  âŒ  â”‚  âœ…  â”‚     âœ…     â”‚    âŒ    â”‚ âœ… OK  â•‘
â•‘ multiply_scalar()   â”‚  âœ…  â”‚   âœ…   â”‚  âŒ  â”‚  âœ…  â”‚     âœ…     â”‚    âœ…    â”‚ âœ… OK  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                       FUNÃ‡Ã•ES DE ATIVAÃ‡ÃƒO (ACTIVATION)                             â•‘
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â•‘ abs()               â”‚  âœ…  â”‚   âœ…   â”‚  âŒ  â”‚  âœ…  â”‚     âœ…     â”‚    âœ…    â”‚ âœ… OK  â•‘
â•‘ relu()              â”‚  âŒ  â”‚   âœ…   â”‚  âŒ  â”‚  âœ…  â”‚     âœ…     â”‚    âœ…    â”‚ âš ï¸ GAP â•‘
â•‘ sigmoid()           â”‚  âŒ  â”‚   âœ…   â”‚  âŒ  â”‚  âœ…  â”‚     âœ…     â”‚    âŒ    â”‚ âš ï¸ GAP â•‘
â•‘ tanh()              â”‚  âŒ  â”‚   âœ…   â”‚  âŒ  â”‚  âœ…  â”‚     âœ…     â”‚    âœ…    â”‚ âš ï¸ GAP â•‘
â•‘ exp()               â”‚  âŒ  â”‚   âœ…   â”‚  âŒ  â”‚  âœ…  â”‚     âœ…     â”‚    âœ…    â”‚ âš ï¸ GAP â•‘
â•‘ log()               â”‚  âŒ  â”‚   âœ…   â”‚  âŒ  â”‚  âœ…  â”‚     âœ…     â”‚    âœ…    â”‚ âš ï¸ GAP â•‘
â•‘ pow()               â”‚  âŒ  â”‚   âœ…   â”‚  âŒ  â”‚  âœ…  â”‚     âŒ     â”‚    âœ…    â”‚ âš ï¸ GAP â•‘
â•‘ sqrt()              â”‚  âœ…  â”‚   âœ…   â”‚  âŒ  â”‚  âœ…  â”‚     âŒ     â”‚    âœ…    â”‚ âœ… OK  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                            OPERAÃ‡Ã•ES MATRICIAIS                                    â•‘
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â•‘ matmul()            â”‚  âŒ  â”‚   âŒ   â”‚  âœ…  â”‚  â“  â”‚     âŒ     â”‚    âœ…    â”‚ âš ï¸ GAP â•‘
â•‘ dot()               â”‚  âŒ  â”‚   âœ…   â”‚  â“  â”‚  âŒ  â”‚     âŒ     â”‚    âœ…    â”‚ âš ï¸ GAP â•‘
â•‘ reshape()           â”‚  âŒ  â”‚   âŒ   â”‚  âŒ  â”‚  âŒ  â”‚     âŒ     â”‚    âŒ    â”‚ âš ï¸ GAP â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                             REDUÃ‡Ã•ES (REDUCTIONS)                                  â•‘
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â•‘ sum()               â”‚  âœ…  â”‚   âœ…   â”‚  âŒ  â”‚  â“  â”‚     âŒ     â”‚    âœ…    â”‚ âœ… OK  â•‘
â•‘ mean()              â”‚  âœ…  â”‚   âœ…   â”‚  âŒ  â”‚  â“  â”‚     âŒ     â”‚    âœ…    â”‚ âœ… OK  â•‘
â•‘ max()               â”‚  âœ…  â”‚   âœ…   â”‚  âŒ  â”‚  â“  â”‚     âŒ     â”‚    âœ…    â”‚ âœ… OK  â•‘
â•‘ min()               â”‚  âŒ  â”‚   âœ…   â”‚  âŒ  â”‚  â“  â”‚     âŒ     â”‚    âœ…    â”‚ âš ï¸ GAP â•‘
â•‘ std()               â”‚  âŒ  â”‚   âœ…   â”‚  âŒ  â”‚  â“  â”‚     âŒ     â”‚    âœ…    â”‚ âš ï¸ GAP â•‘
â•‘ soma(axis)          â”‚  âŒ  â”‚   âœ…   â”‚  âŒ  â”‚  âŒ  â”‚     âŒ     â”‚    âœ…    â”‚ âš ï¸ GAP â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                        OPERAÃ‡Ã•ES ESPECIALIZADAS                                    â•‘
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â•‘ relu_derivative()   â”‚  âŒ  â”‚   âœ…   â”‚  âŒ  â”‚  âŒ  â”‚     âŒ     â”‚    âœ…    â”‚ âš ï¸ GAP â•‘
â•‘ sigmoid_derivative()â”‚  âŒ  â”‚   âœ…   â”‚  âŒ  â”‚  âŒ  â”‚     âŒ     â”‚    âœ…    â”‚ âš ï¸ GAP â•‘
â•‘ softmax()           â”‚  âŒ  â”‚   âŒ   â”‚  âŒ  â”‚  âŒ  â”‚     âŒ     â”‚    âŒ    â”‚ âŒ NOP â•‘
â•‘ softmax_derivative()â”‚  âŒ  â”‚   âŒ   â”‚  âŒ  â”‚  âŒ  â”‚     âŒ     â”‚    âŒ    â”‚ âŒ NOP â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Legenda:
âœ… = Implementado e Otimizado
âŒ = NÃ£o implementado
â“ = ProvÃ¡vel (precisa verificar gpu_wrapper.cu)
âš ï¸ GAP = Oportunidade de melhoria identificada
```

---

## ğŸ”§ ConfiguraÃ§Ãµes Importantes

### Thresholds PadrÃ£o

| Constante | Valor | PropÃ³sito | Ajuste |
|-----------|-------|----------|--------|
| `ZMATRIX_PARALLEL_THRESHOLD` | 40,000 | Min elementos para paralelizar com OpenMP | â†‘ Reduzir se <<40 cores |
| `ZMATRIX_GPU_THRESHOLD` | 200,000 | Min elementos para usar GPU | â†‘ Aumentar se GPU lenta |

**RecomendaÃ§Ãµes por Hardware:**

```
CPU apenas (multi-core):    ZMATRIX_PARALLEL_THRESHOLD = 20,000
CPU + GPU:                  ZMATRIX_PARALLEL_THRESHOLD = 10,000
                            ZMATRIX_GPU_THRESHOLD     = 100,000

NUMA systems:               ZMATRIX_PARALLEL_THRESHOLD = 50,000
```

### VariÃ¡veis de Ambiente

```bash
# Debug GPU execution
export ZMATRIX_GPU_DEBUG=1

# ForÃ§ar CPU mesmo com GPU disponÃ­vel
export ZMATRIX_FORCE_CPU=1
```

---

## ğŸ“Š Performance Actual vs Esperada

### Hardware TÃ­pico: CPU Intel Core i7-10700K + RTX 3070

```
OperaÃ§Ã£o          Tamanho    Sem OtimizaÃ§Ãµes    Com OtimizaÃ§Ãµes    Speedup
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
add()             10M        380ms              42ms               9.0x
relu()            10M        520ms              175ms              3.0x
exp()             10M        850ms              280ms              3.0x
matmul()          1000Ã—1000  280ms              28ms              10.0x
```

---

## ğŸ¯ Checklist de CompilaÃ§Ã£o

### Configure Flags Recomendados

```bash
# Build optimal com todas as otimizaÃ§Ãµes
./configure \
    --enable-cuda \
    --with-cuda-path=/usr/local/cuda \
    --enable-openmp \
    --enable-simd \
    --with-cblas \
    --with-cflags="-O3 -march=native -mavx2"

# Build conservador (compatibilidade mÃ¡xima)
./configure \
    --enable-cuda \
    --enable-openmp \
    --disable-simd \
    --with-cflags="-O2"
```

### VerificaÃ§Ã£o PÃ³s-Build

```bash
# Testar se SIMD foi incluÃ­do
nm libzmatrix.so | grep simd_dispatch
# SaÃ­da esperada: sÃ­mbolos de simd_dispatch.h

# Testar se CUDA foi compilado
nm libzmatrix.so | grep gpu_
# SaÃ­da esperada: gpu_add, gpu_relu, etc.

# Testar se OpenMP foi incluÃ­do
ldd libzmatrix.so | grep omp
# SaÃ­da esperada: libomp.so ou libgomp.so

# Debug: enable CUDA debug
ZMATRIX_GPU_DEBUG=1 php test.php
```

---

## ğŸ” Diagnostic Commands

### Verificar Capacidades SIMD

```bash
# Check CPU flags
cat /proc/cpuinfo | grep -E "avx|avx2|avx512"

# Ou no macOS:
sysctl -a | grep -i "avx"

# Build com diagnostic
php -r "echo phpversion('zmatrix') . PHP_EOL;"
```

### Benchmark Individual

```php
<?php
$a = new ZTensor([1000000]);
$a->fill(2.0);

// Benchmark add()
$start = microtime(true);
for ($i = 0; $i < 1000; $i++) {
    $b = $a->add($a);
}
echo "add(1M): " . (microtime(true) - $start) . "s\n";

// Benchmark relu()
$start = microtime(true);
for ($i = 0; $i < 1000; $i++) {
    $a->relu();
}
echo "relu(1M): " . (microtime(true) - $start) . "s\n";
```

---

## ğŸ“š Arquivo de ReferÃªncia RÃ¡pida

### Locais-chave no CÃ³digo

| Componente | Arquivo | PropÃ³sito |
|-----------|---------|----------|
| Thresholds | `src/zmatrix.cpp` | Configurar limites |
| SIMD Dispatch | `src/simd/simd_dispatch.h` | ImplementaÃ§Ãµes SIMD |
| GPU Wrapper | `src/gpu_wrapper.h` | Interface CUDA |
| GPU Implementation | `src/gpu_wrapper.cu` | Kernels CUDA |
| ZTensor Struct | `src/zmatrix.cpp` | DefiniÃ§Ã£o da classe |
| MÃ©todos Arith. | `src/zmatrix.cpp` | add, mul, subtract, etc |
| AtivaÃ§Ãµes | `src/zmatrix.cpp` | relu, sigmoid, exp, etc |
| ReduÃ§Ãµes | `src/zmatrix.cpp` | sum, mean, max, std |

---

## âš¡ Troubleshooting

### Erro: "GPU threshold too low"
**Causa:** GPU menos rÃ¡pida que CPU para pequenos tensores  
**SoluÃ§Ã£o:**
```bash
# Aumentar threshold no cÃ³digo ou:
export ZMATRIX_GPU_THRESHOLD=500000
```

### Erro: "SIMD not available"
**Causa:** CPU sem AVX2  
**SoluÃ§Ã£o:** Compilar sem `-march=native`
```bash
./configure --with-cflags="-O2"
```

### Erro: "CUDA out of memory"
**Causa:** Tensor grande demais para GPU  
**SoluÃ§Ã£o:** Reduzir tensores ou usar CPU:
```bash
export ZMATRIX_FORCE_CPU=1
```

### Performance ruim em matmul
**Causa:** BLAS nÃ£o otimizado ou nÃ£o compilado  
**SoluÃ§Ã£o:** Verificar BLAS installation
```bash
dpkg -l | grep -i blas  # Linux
# Deve mostrar: libblas, liblapack, libopenblas
```

---

## ğŸ”— Matriz de Suporte

| TÃ©cnica | Suporte | Fallback | Status |
|---------|---------|----------|--------|
| SIMD (bÃ¡sico) | AVX2/AVX-512 | Loop sequencial | âœ… Full |
| OpenMP | GCC/LLVM/MSVC | Sem threads | âœ… Full |
| BLAS | OpenBLAS/MKL/Netlib | Loop manual | âš ï¸ Parcial |
| CUDA | NVIDIA GPU | CPU mode | âœ… Full |
| AVX2 | Modern CPUs (2013+) | SSE/Scalar | âœ… Full |
| AVX-512 | Xeon/i7-11th+ | AVX2 | âœ… Full |

---

## ğŸ“ˆ Ganhos de Performance por Categoria

### 1. OperaÃ§Ãµes Elemento-a-Elemento (add, mul, etc)
- **CPU com SIMD+OpenMP:** 5-10x vs. baseline
- **GPU:** 15-50x vs. CPU base
- **Melhor para:** Arrays > 100K elementos

### 2. FunÃ§Ãµes de AtivaÃ§Ã£o (relu, exp, tanh)
- **CPU com OpenMP:** 2-4x vs. baseline
- **CPU com SIMD:** 3-4x vs. baseline (se implementado)
- **GPU:** 8-15x vs. CPU base
- **Melhor para:** Redes neurais profundas

### 3. Matrix Multiplication (matmul)
- **CPU com BLAS:** 5-20x vs. baseline
- **GPU com cuBLAS:** 5-10x vs. BLAS
- **Melhor para:** OperaÃ§Ãµes de > 1000Ã—1000

### 4. ReduÃ§Ãµes (sum, mean, std)
- **CPU com OpenMP:** 3-6x vs. baseline
- **CPU com SIMD:** 2-4x vs. baseline (se implementado)
- **GPU:** 10-20x vs. CPU base
- **Melhor para:** OperaÃ§Ãµes normalizadoras

---

## ğŸ“ Leitura Recomendada

- [Eigen Library](https://eigen.tuxfamily.org/) - ReferÃªncia para SIMD dispatch
- [OpenBLAS Docs](https://github.com/xianyi/OpenBLAS/wiki) - BLAS optimization
- [NVIDIA CUDA Documentation](https://docs.nvidia.com/cuda/) - GPU computing
- [Intel Intrinsics Guide](https://www.intel.com/content/www/en/en/docs/intrinsics-guide/index.html) - AVX intrinsics
- [GCC OpenMP](https://gcc.gnu.org/projects/gomp/) - OpenMP pragma

---

*Quick Reference - 17 de Janeiro de 2026*  
*v1.0 - Reference Edition*
