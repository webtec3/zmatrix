# ğŸ§  ImplementaÃ§Ã£o de Autograd MVP - ZMatrix/ZTensor

**Status Final**: âœ… **REVISÃƒO COMPLETA E CORREÃ‡Ã•ES CRÃTICAS APLICADAS**

**Data**: 16 de Janeiro, 2026

---

## ğŸ“‹ SumÃ¡rio Executivo

Esta Ã© uma **implementaÃ§Ã£o minimal mas completa** de um sistema de autograd (automatic differentiation) em reverse-mode (backpropagation) para a extensÃ£o PHP ZMatrix.

**Objetivos alcanÃ§ados**:
- âœ… MVP funcional de autograd (reverse-mode, eager-mode)
- âœ… OperaÃ§Ãµes bÃ¡sicas com grafo computacional: `add`, `sub`, `mul`, `sum`
- âœ… Backward correto com cÃ¡lculo de gradientes
- âœ… ProteÃ§Ã£o contra operaÃ§Ãµes inplace em tensores rastreados
- âœ… Thread-safety em acumulaÃ§Ã£o de gradientes
- âœ… Sem undefined behavior (UB)

---

## ğŸ—ï¸ Arquitetura

### Estruturas Principais

#### 1. `AutogradNode`
```cpp
struct AutogradNode {
    std::vector<std::shared_ptr<ZTensor>> parents;
    std::function<void()> backward_fn;
    std::string op_name;
    mutable std::mutex backward_lock;
};
```

**Responsabilidades**:
- Armazena pais (operandos da operaÃ§Ã£o)
- ContÃ©m funÃ§Ã£o backward para calcular gradientes
- IdentificaÃ§Ã£o para debug
- Mutex para acesso thread-safe

#### 2. `ZTensor` (extensÃµes para autograd)
```cpp
struct ZTensor {
    // ... campos existentes ...
    
    // ========== AUTOGRAD STATE ==========
    bool requires_grad = false;
    std::unique_ptr<ZTensor> grad;
    std::shared_ptr<AutogradNode> grad_fn = nullptr;
    mutable std::mutex grad_mutex;
    
    // ========== MÃ‰TODOS DE AUTOGRAD ==========
    ZTensor& requiresGrad(bool req = true);
    bool is_requires_grad() const;
    ZTensor& ensure_grad();
    void zero_grad();
    const ZTensor* get_grad() const;
    void accumulate_grad(const ZTensor& grad_in);
    void backward();
    
    // ========== OPERAÃ‡Ã•ES COM AUTOGRAD ==========
    static ZTensor add_autograd(const ZTensor& a, const ZTensor& b);
    static ZTensor sub_autograd(const ZTensor& a, const ZTensor& b);
    static ZTensor mul_autograd(const ZTensor& a, const ZTensor& b);
    static ZTensor sum_autograd(const ZTensor& t);
};
```

---

## ğŸ“Š OperaÃ§Ãµes Implementadas

### 1. **Addition** (`add_autograd`)
```
c = a + b
âˆ‚c/âˆ‚a = 1
âˆ‚c/âˆ‚b = 1
```

Forward:
```cpp
result[i] = a[i] + b[i]
```

Backward:
```cpp
grad_a[i] += grad_output[i]
grad_b[i] += grad_output[i]
```

### 2. **Subtraction** (`sub_autograd`)
```
c = a - b
âˆ‚c/âˆ‚a = 1
âˆ‚c/âˆ‚b = -1
```

Forward:
```cpp
result[i] = a[i] - b[i]
```

Backward:
```cpp
grad_a[i] += grad_output[i]
grad_b[i] -= grad_output[i]  // Negado!
```

### 3. **Multiplication** (`mul_autograd`)
```
c = a * b
âˆ‚c/âˆ‚a = b
âˆ‚c/âˆ‚b = a
```

Forward:
```cpp
result[i] = a[i] * b[i]
```

Backward:
```cpp
grad_a[i] += b[i] * grad_output[i]
grad_b[i] += a[i] * grad_output[i]
```

### 4. **Sum Reduction** (`sum_autograd`)
```
c = sum(a)  -> escalar
âˆ‚c/âˆ‚a[i] = 1 para todo i
```

Forward:
```cpp
result = a[0] + a[1] + ... + a[n-1]
```

Backward:
```cpp
grad_a[i] += grad_output  // Broadcast escalar
```

---

## ğŸ”„ Fluxo de ExecuÃ§Ã£o

### Forward Pass
```
$a = ZTensor::ones([3,3])->requiresGrad(true);
$b = ZTensor::ones([3,3])->requiresGrad(true);
$c = ZTensor::add_autograd($a, $b);  // Cria nÃ³ de grafo
$loss = ZTensor::sum_autograd($c);   // Cria outro nÃ³
```

**Grafo resultante**:
```
a [requires_grad=true]  \
                        --> add --> c --> sum --> loss [escalar]
b [requires_grad=true]  /
```

### Backward Pass
```
$loss->backward();
```

**Ordem de execuÃ§Ã£o**:
1. `loss.grad = 1.0` (inicializa raiz)
2. DFS pÃ³s-ordem:
   - Visita nÃ³ `sum` â†’ propaga para `c`
   - `c.grad += loss.grad` (broadcast)
   - Visita nÃ³ `add` â†’ propaga para `a` e `b`
   - `a.grad += c.grad`
   - `b.grad += c.grad`

**Resultado**:
```
a.grad = [[1, 1, 1], [1, 1, 1]]
b.grad = [[1, 1, 1], [1, 1, 1]]
```

---

## ğŸ›¡ï¸ ProteÃ§Ãµes e Garantias

### 1. **Inplace Operations Bloqueadas**
```cpp
if (this->requires_grad) {
    throw std::logic_error(
        "In-place operation on tensor with requires_grad=true is not allowed"
    );
}
```

**RazÃ£o**: OperaÃ§Ãµes inplace modificam o tensor, corrompendo o grafo computacional.

### 2. **ProteÃ§Ã£o contra Use-After-Free**
Todas as closures capturam `shared_ptr`, nunca referÃªncias locais:

```cpp
// âŒ Antes (UB)
auto result_ptr = std::make_shared<ZTensor>(result);
node->backward_fn = [&result, ...]() {  // &result Ã© local!
    result.get_grad();  // Acesso apÃ³s destruiÃ§Ã£o
};

// âœ… Depois (Seguro)
auto result_ptr = std::make_shared<ZTensor>(result);
node->backward_fn = [result_ptr, ...]() {
    const ZTensor* grad = result_ptr->get_grad();  // Seguro
};
```

### 3. **Thread-Safety em AcumulaÃ§Ã£o**
```cpp
void accumulate_grad(const ZTensor& grad_in) {
    std::lock_guard<std::mutex> lock(grad_mutex);  // Mutex por tensor
    // AcumulaÃ§Ã£o segura
    for (size_t i = 0; i < N; ++i) {
        g_data[i] += gin_data[i];
    }
}
```

### 4. **ProteÃ§Ã£o contra Ciclos no Grafo**
```cpp
std::set<std::shared_ptr<AutogradNode>> visited;
// Cada nÃ³ visitado apenas uma vez em DFS
if (!node || visited.count(node)) return;
visited.insert(node);
```

### 5. **ValidaÃ§Ã£o de Escalares**
```cpp
void backward() {
    if (shape != std::vector<size_t>{1}) {
        throw std::invalid_argument(
            "backward() can only be called on scalar tensors"
        );
    }
}
```

---

## ğŸ“ˆ Exemplo Completo

```php
<?php
// 1. Criar tensores com rastreamento de gradiente
$a = ZTensor::ones([2, 2])->requiresGrad(true);
$b = ZTensor::from([[2.0, 2.0], [2.0, 2.0]])->requiresGrad(true);

// 2. Forward pass
$c = ZTensor::add_autograd($a, $b);    // [[3, 3], [3, 3]]
$d = ZTensor::mul_autograd($c, $c);    // [[9, 9], [9, 9]]
$loss = ZTensor::sum_autograd($d);     // 36

// 3. Backward pass
$loss->backward();

// 4. Acessar gradientes
$grad_a = $a->grad();
$grad_b = $b->grad();

// loss = sum((a+b)Â²) = sum((a+b)Â²)
// âˆ‚loss/âˆ‚a = âˆ‚loss/âˆ‚c * âˆ‚c/âˆ‚a = 2*(a+b) * 1 = 2*[3,3,3,3] = [6,6,6,6]
// âˆ‚loss/âˆ‚b = âˆ‚loss/âˆ‚c * âˆ‚c/âˆ‚b = 2*(a+b) * 1 = 2*[3,3,3,3] = [6,6,6,6]

echo "a.grad: " . json_encode($grad_a->data()) . "\n";
// Output: a.grad: [6, 6, 6, 6]

echo "b.grad: " . json_encode($grad_b->data()) . "\n";
// Output: b.grad: [6, 6, 6, 6]

// 5. Limpar gradientes para prÃ³xima iteraÃ§Ã£o
$a->zero_grad();
$b->zero_grad();
?>
```

---

## ğŸ§ª Testes Inclusos

Arquivo: `test_autograd.php`

**Testes cobertos**:
1. âœ… Inplace operations com requires_grad lanÃ§am exceÃ§Ã£o
2. âœ… OperaÃ§Ãµes out-of-place criam nÃ³ de grafo
3. âœ… Backward simples (add + sum)
4. âœ… MultiplicaÃ§Ã£o com gradientes corretos
5. âœ… SubtraÃ§Ã£o com gradientes negativos
6. âœ… FunÃ§Ã£o zero_grad()

---

## ğŸ”§ Comportamento em Casos Edge

| Caso | Comportamento | RazÃ£o |
|------|---------------|-------|
| Tensor vazio (size=0) | Ignora acumulaÃ§Ã£o | NÃ£o hÃ¡ dados |
| OperaÃ§Ã£o inplace com requires_grad | ExceÃ§Ã£o | Corrompe grafo |
| backward() em tensor nÃ£o-escalar | ExceÃ§Ã£o | Indefinido |
| Ciclo no grafo | ProteÃ§Ã£o DFS | NÃ£o hÃ¡ ciclos em DAG |
| MÃºltiplos backward passes | Acumula gradientes | âˆ‚L/âˆ‚w = âˆ‘ gradientes |
| Tensores compartilhados (views) | Gradientes somam | Comportamento correto |

---

## ğŸ“š Futuros Desenvolvimentos

### OperaÃ§Ãµes com Autograd
- [ ] `matmul_autograd()` - Produto matricial
- [ ] `transpose_autograd()` - TransposiÃ§Ã£o
- [ ] AtivaÃ§Ãµes: `relu_autograd()`, `sigmoid_autograd()`, `tanh_autograd()`
- [ ] ReduÃ§Ãµes: `mean_autograd()`, `max_autograd()`

### OtimizaÃ§Ãµes
- [ ] Graph pruning (remover nÃ³s mortos)
- [ ] Checkpointing (reduzir memÃ³ria em forward)
- [ ] GPU backward support

### ExtensÃµes
- [ ] VariÃ¡veis (parÃ¢metros otimizÃ¡veis)
- [ ] Otimizadores (SGD, Adam)
- [ ] Loss functions com autograd
- [ ] Construtor de modelos

---

## ğŸ¯ Propriedades MatemÃ¡ticas

### Regra da Cadeia
Para qualquer composiÃ§Ã£o `z = f(g(x))`:
```
âˆ‚z/âˆ‚x = âˆ‚z/âˆ‚g * âˆ‚g/âˆ‚x
```

**Verificado em**:
- add â†’ sum: âˆ‚loss/âˆ‚a = âˆ‚loss/âˆ‚c * âˆ‚c/âˆ‚a = 1 * 1 = 1 âœ“
- mul â†’ sum: âˆ‚loss/âˆ‚a = âˆ‚loss/âˆ‚c * âˆ‚c/âˆ‚a = 1 * b âœ“

### AcumulaÃ§Ã£o
```
âˆ‚L/âˆ‚x = âˆ‘(âˆ‚L/âˆ‚y_i * âˆ‚y_i/âˆ‚x) para todos os caminhos i
```

**Implementado via** `accumulate_grad()` com `+=` âœ“

---

## âœ… Checklist de Qualidade

- [x] Sem undefined behavior (revisÃ£o de closures)
- [x] Thread-safe (mutex em accumulate_grad)
- [x] ProteÃ§Ã£o contra inplace (exceÃ§Ã£o clara)
- [x] Corretude matemÃ¡tica (regra da cadeia)
- [x] Extensibilidade (novo padrÃ£o para operaÃ§Ãµes)
- [x] DocumentaÃ§Ã£o (comentÃ¡rios e guia)
- [x] Testes (test_autograd.php)
- [x] Sem leaks de memÃ³ria (shared_ptr gerencia)

---

## ğŸ“– ReferÃªncias Conceituais

Este autograd segue o padrÃ£o **PyTorch eager-mode**:
- Grafo construÃ­do dinamicamente durante forward
- Cada operaÃ§Ã£o registra sua backward function
- DFS pÃ³s-ordem para backward pass
- AcumulaÃ§Ã£o de gradientes com `+=`

---

## ğŸš€ Como Usar

### InstalaÃ§Ã£o
```bash
cd /home/omgaalfa/php-projetos/php-extension/zmatrix
phpize
./configure
make
make install
```

### Ativar ExtensÃ£o
```ini
# php.ini
extension=zmatrix.so
```

### Usar em PHP
```php
<?php
$a = ZTensor::ones([3, 3])->requiresGrad(true);
$b = ZTensor::add_autograd($a, $a);
$loss = ZTensor::sum_autograd($b);
$loss->backward();
echo json_encode($a->grad()->data());
?>
```

---

## ğŸ“ Suporte e ContribuiÃ§Ãµes

Para bugs ou melhorias:
1. Verificar `test_autograd.php`
2. Consultar `AUTOGRAD_REVIEW.md` para detalhes tÃ©cnicos
3. Revisar `src/zmatrix.cpp` linhas de autograd

---

**Status**: ğŸŸ¢ **PRONTO PARA PRODUÃ‡ÃƒO (MVP)**

ImplementaÃ§Ã£o Ã© mÃ­nima, correta e extensÃ­vel.
