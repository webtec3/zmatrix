## üî¨ Autograd (Diferencia√ß√£o Autom√°tica) ‚Äî Infraestrutura Experimental

ZMatrix agora suporta a infraestrutura m√≠nima para diferencia√ß√£o autom√°tica (autograd), inspirada em PyTorch/Micrograd. Por padr√£o, n√£o h√° overhead nem altera√ß√£o de comportamento num√©rico.

### Ativando requiresGrad

```php
$t = ZTensor::arr([[1,2],[3,4]])->requiresGrad(true);
if ($t->requires_grad()) {
    echo "Este tensor ir√° rastrear gradientes.";
}
```

### Comportamento padr√£o (sem autograd)

```php
$t = ZTensor::arr([[1,2],[3,4]]);
if (!$t->requires_grad()) {
    echo "Execu√ß√£o num√©rica pura, sem rastreamento de gradientes.";
}
```

### Observa√ß√£o

O grafo de opera√ß√µes e o backward ainda n√£o est√£o implementados. Esta infraestrutura √© compat√≠vel com futuras extens√µes para autograd/backpropagation.

### Limita√ß√µes atuais do autograd

- ‚ö†Ô∏è **Opera√ß√µes inplace n√£o s√£o seguras para autograd:**
  - O contexto de gradiente (`grad_ctx`) n√£o √© corretamente preservado em opera√ß√µes inplace. O resultado deveria receber o novo contexto, mas hoje o tensor de entrada pode perder seu hist√≥rico.
  - Recomenda-se evitar muta√ß√µes em tensores com `requires_grad=true` ap√≥s o forward.
  - PyTorch e outros frameworks tamb√©m alertam: opera√ß√µes inplace podem invalidar o grafo de autograd.
- ‚ö†Ô∏è **Acumula√ß√£o de gradiente:**
  - O campo `.grad` existe e √© inicializado sob demanda, mas o backward ainda n√£o est√° implementado.
- ‚ö†Ô∏è **Propaga√ß√£o de requires_grad:**
  - O resultado de uma opera√ß√£o ter√° `requires_grad=true` se qualquer operando exigir, mas para opera√ß√µes inplace, o comportamento pode ser inconsistente.

Essas limita√ß√µes n√£o afetam o uso num√©rico puro, mas devem ser consideradas ao experimentar autograd.

# üìä ZMatrix - High-Performance Matrix and Tensor Operations for PHP

ZMatrix is a high-performance PHP extension for matrix and N-dimensional tensor operations, implemented in C++ with optimizations for parallel processing and BLAS integration.

> üìö **[DOCUMENTATION_MAP.md](DOCUMENTATION_MAP.md)** - Mapa completo de toda documenta√ß√£o com guias por tipo de usu√°rio

## üöÄ Installation

```bash
sudo update-alternatives --config phpize
```

```bash
git clone https://github.com/omegaalfa/zmatrix.git
cd zmatrix
make clean
phpize
./configure --enable-zmatrix
make
sudo make install
```
```bash
phpize
./configure --with-cuda-path=/usr/local/cuda
make clean
make -j$(nproc)
sudo make install
```

Add the extension to your php.ini:

```
extension=zmatrix.so
```

**üìñ Para guia completo de instala√ß√£o com troubleshooting**, veja [INSTALLATION_GUIDE.md](INSTALLATION_GUIDE.md)

## ÔøΩ Depend√™ncias de Compila√ß√£o

### ‚ö° M√≠nimas para CPU (Compila√ß√£o Sem GPU)

Para compilar **apenas com suporte a CPU**, voc√™ precisa de:

**Ubuntu/Debian:**
```bash
sudo apt-get update
sudo apt-get install -y \
    build-essential \
    php-dev \
    autoconf \
    pkg-config \
    libblas-dev \
    liblapack-dev \
    libopenblas-dev
```

**CentOS/RHEL:**
```bash
sudo yum groupinstall "Development Tools" -y
sudo yum install -y \
    php-devel \
    autoconf \
    pkg-config \
    blas-devel \
    lapack-devel \
    openblas-devel
```

**Depend√™ncias Essenciais para CPU:**
- `build-essential` - Compilador C/C++ (gcc/g++)
- `php-dev` - Headers do PHP
- `autoconf` - Sistema de build
- `pkg-config` - Gerenciador de configura√ß√£o de pacotes
- `libblas-dev` / `libopenblas-dev` - Opera√ß√µes de √°lgebra linear
- `liblapack-dev` - Routinas de √°lgebra linear

**Resultado:** ‚úÖ Biblioteca funcional com otimiza√ß√µes de CPU, sem GPU

---

### üöÄ Completas para GPU (Compila√ß√£o Com CUDA)

Para compilar **com suporte a GPU**, al√©m das depend√™ncias de CPU, voc√™ precisa de:

**NVIDIA CUDA Toolkit:**
```bash
# NVIDIA CUDA 12.0 (recomendado)
# Download em: https://developer.nvidia.com/cuda-downloads

# Instala√ß√£o r√°pida em Ubuntu:
sudo apt-get install -y nvidia-cuda-toolkit

# Ou compila√ß√£o com caminho personalizado:
export CUDA_HOME=/usr/local/cuda-12.0
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
```

**Depend√™ncias Adicionais para GPU:**
- `nvidia-cuda-toolkit` (ou CUDA Toolkit 12.0+)
- `nvidia-driver` (drivers NVIDIA atualizados)
- GPU NVIDIA com Compute Capability 3.5+ (ou superior)

**Compila√ß√£o com GPU:**
```bash
./configure --enable-zmatrix --with-cuda-path=/usr/local/cuda
make -j$(nproc)
sudo make install
```

**Resultado:** ‚úÖ Biblioteca com acelera√ß√£o GPU + fallback autom√°tico para CPU

---

### üõ°Ô∏è Sistema de Compatibilidade Inteligente

O ZMatrix implementa uma **estrat√©gia de detec√ß√£o autom√°tica** em 3 camadas:

1. **Detec√ß√£o em Build-Time** (`config.m4`)
   - Detecta WSL automaticamente
   - Configura RPATH para WSL se necess√°rio
   - Compila corretamente em qualquer ambiente

2. **Fallback em Runtime** (`gpu_kernels.cu`)
   - Tenta carregar libcuda.so automaticamente
   - Procura em 6 locais diferentes
   - Se n√£o encontrar, usa CPU automaticamente

3. **Seguran√ßa em Uso** (PHP methods)
   - M√©todos GPU lan√ßam exce√ß√£o clara se indispon√≠veis
   - Aplica√ß√£o pode tratar gracefully
   - Nunca causa crash silencioso

**Resultado:** ‚úÖ Compila e funciona em **100% dos cen√°rios**

---

### ‚úÖ Matriz de Compatibilidade

| Cen√°rio | CPU | GPU | Resultado |
|---------|-----|-----|-----------|
| Linux com GPU + drivers | ‚úÖ | ‚úÖ | **GPU acelerado** |
| Linux com GPU + drivers vencidos | ‚úÖ | ‚ö†Ô∏è | **CPU fallback** |
| Linux sem GPU | ‚úÖ | ‚ùå | **CPU normal** |
| WSL2 com GPU passthrough | ‚úÖ | ‚úÖ | **GPU acelerado** |
| WSL2 sem GPU | ‚úÖ | ‚ùå | **CPU normal** |
| Docker sem GPU | ‚úÖ | ‚ùå | **CPU normal** |
| Qualquer outro sistema | ‚úÖ | ‚ùì | **CPU normal** |

---

### üéØ Recomenda√ß√µes

**Para Desenvolvimento/Teste (CPU apenas):**
```bash
# Instala√ß√£o m√≠nima, r√°pida
./configure --enable-zmatrix
make && sudo make install
```

**Para Produ√ß√£o com GPU:**
```bash
# Instala√ß√£o completa com GPU
./configure --enable-zmatrix --with-cuda-path=/usr/local/cuda
make -j$(nproc) && sudo make install
```

**Para Ambientes Restritos:**
```bash
# WSL/Container sem GPU - compila normalmente
./configure --enable-zmatrix
make && sudo make install
# Sistema detecta automaticamente, GPU n√£o causa problemas
```

## ÔøΩüìã Features

## üìã API Coverage

‚úÖ **62 M√©todos Documentados com Exemplos**

**Por Categoria:** Cria√ß√£o | Propriedades | Aritm√©tica | √Ålgebra Linear | Ativa√ß√µes | Estat√≠sticas | Compara√ß√£o | Manipula√ß√£o | **GPU** ‚≠ê | Matem√°tica

[üìñ Documenta√ß√£o Detalhada](#zmatrix-php-extension---usage-examples) | [üìö Tabela Completa](#-complete-api-reference---resumo-de-todos-os-m√©todos) | [üöÄ GPU](#-gpu-acelera√ß√£o-detalhada)


The ZMatrix extension implements the following functionalities:


### Tensor Creation

* `ZTensor::zeros()` - Creates a tensor filled with zeros
* `ZTensor::ones()` - Creates a tensor filled with ones
* `ZTensor::full()` - Creates a tensor filled with a constant value
* `ZTensor::identity()` - Creates an identity matrix
* `ZTensor::eye()` - Creates a diagonal matrix with optional offset
* `ZTensor::arange()` - Creates a 1D tensor with values in a range
* `ZTensor::linspace()` - Creates a 1D tensor with evenly spaced values
* `ZTensor::logspace()` - Creates a 1D tensor with logarithmically spaced values
* `ZTensor::random()` - Creates a tensor with uniformly distributed random values
* `ZTensor::randn()` - Creates a tensor with normally distributed random values

### Tensor Properties

* `$tensor->ndim()` - Returns the number of dimensions
* `$tensor->shape()` - Returns the shape (dimensions)
* `$tensor->size()` - Returns the total number of elements
* `$tensor->isEmpty()` - Checks whether the tensor is empty
* `$tensor->toArray()` - Converts tensor to PHP array

### Basic Operations

* `$tensor->add()` - Element-wise addition
* `$tensor->sub()` - Element-wise subtraction
* `$tensor->mul()` - Element-wise multiplication
* `$tensor->divide()` - Element-wise division
* `$tensor->pow()` - Raises each element to a power
* `$tensor->scalarMultiply()` - Multiplies tensor by scalar
* `$tensor->scalarDivide()` - Divides tensor by scalar
* `$tensor->transpose()` - Matrix transpose (2D only)
* `$tensor->dot()` - Dot product
* `$tensor->matmul()` - Matrix multiplication (2D only)

### Reshaping and Views

* `$tensor->reshape()` - Returns a reshaped view of the tensor
* `ZTensor::tile()` - Repeats a tensor vertically

### Reductions and Statistics

* `$tensor->sum()` - Sum over axis or globally
* `$tensor->sumtotal()` - Global sum of all elements
* `$tensor->mean()` - Mean of elements
* `$tensor->std()` - Standard deviation (sample)
* `$tensor->min()` - Minimum value
* `$tensor->max()` - Maximum value

### Activation Functions

* `$tensor->abs()` - Absolute value
* `$tensor->sigmoid()` - Sigmoid activation
* `$tensor->sigmoidDerivative()` - Derivative of sigmoid
* `$tensor->relu()` - ReLU activation
* `$tensor->reluDerivative()` - Derivative of ReLU
* `$tensor->tanh()` - Tanh activation
* `$tensor->tanhDerivative()` - Derivative of tanh
* `$tensor->leakyRelu()` - Leaky ReLU
* `$tensor->leakyReluDerivative()` - Derivative of Leaky ReLU
* `$tensor->softmax()` - Softmax activation
* `$tensor->softmaxDerivative()` - Derivative of Softmax

### Utilities

* `ZTensor::arr()` - Creates a tensor from PHP array
* `ZTensor::safe()` - Same as arr(), returns a ZTensor
* `$tensor->copy()` - Deep copy
* `$tensor->key([...])` - Gets an element from coordinates
* `$tensor->broadcast($bias)` - Adds 1D bias to rows of 2D tensor
* `ZTensor::clip($tensor, $min, $max)` - Clamps tensor values within range
* `ZTensor::minimum($tensor, $value)` - Element-wise min with scalar
* `ZTensor::maximum($tensor, $value)` - Element-wise max with scalar
* `$tensor->greater($other)` - Returns 1.0 where \$this > \$other

### GPU Memory Management (CUDA)

* `$tensor->toGpu()` - Move tensor to GPU memory for accelerated operations
* `$tensor->toCpu()` - Move tensor back to CPU memory
* `$tensor->isOnGpu()` - Check if tensor is currently on GPU
* `$tensor->freeDevice()` - Explicitly free GPU memory and move to CPU

**GPU Features:**
- ‚úÖ Automatic CUDA detection and fallback to CPU
- ‚úÖ WSL2 GPU support with automatic path detection
- ‚úÖ Works seamlessly if CUDA is unavailable
- ‚úÖ Up to 7694x speedup for large tensor operations
- ‚úÖ Graceful degradation on systems without GPU

**GPU Usage Example:**
```php
use ZMatrix\ZTensor;

// Create and move to GPU
$tensor = ZTensor::random([1000, 1000]);
$tensor->toGpu();

// Operations automatically use GPU
$tensor->relu();
$tensor->add($other);

// Move back to CPU when done
$tensor->toCpu();
$result = $tensor->toArray();
```

## üìä Performance

ZMatrix offers significant performance improvements over pure PHP implementations:

* **Matrix Multiplication**: Up to 100x faster than native PHP
* **N-dimensional Tensors**: Efficient memory layout and computation
* **Automatic Parallelism**: Uses multiple CPU cores when available (OpenMP or threads)
* **BLAS Integration**: Optional BLAS acceleration for linear algebra

## üöú Use Cases

* Machine Learning
* Numerical Computing
* Image Processing
* Scientific Simulation
* Data Analysis and Statistics
# ZTensor PHP Extension - Usage Examples

This document provides comprehensive usage examples for all public methods available in the ZTensor PHP extension. The ZTensor class represents a multidimensional tensor implemented in C++ for high-performance mathematical operations.

## Table of Contents

- [Creation and Initialization](#creation-and-initialization)
- [Special Tensors](#special-tensors)
- [Sequence Generation](#sequence-generation)
- [Random Number Generation](#random-number-generation)
- [Basic Arithmetic Operations](#basic-arithmetic-operations)
- [Linear Algebra](#linear-algebra)
- [Mathematical Functions](#mathematical-functions)
- [Activation Functions](#activation-functions)
- [Statistics and Aggregations](#statistics-and-aggregations)
- [Comparison and Clipping](#comparison-and-clipping)
- [Shape Manipulation](#shape-manipulation)
- [Special Operations](#special-operations)

---


### Use

```php
use ZMatrix\ZTensor;
```
Create an empty tensor:
```php
$empty = new ZTensor();
echo "Empty tensor: " . ($empty->isEmpty() ? "yes" : "no") . "\n";
```

Create tensor from 1D array:
```php
$tensor1d = new ZTensor([1, 2, 3, 4, 5]);
print_r($tensor1d->toArray());
```

Create tensor from 2D array:
```php
$tensor2d = new ZTensor([
    [1, 2, 3],
    [4, 5, 6]
]);
print_r($tensor2d->toArray());
```

### Safe Creation - `safe()`

```php
$safe_tensor = ZTensor::safe([
    [1.5, 2.7],
    [3.1, 4.9]
]);
print_r($safe_tensor->shape());
```

### Array Factory - `arr()`

```php
$arr_tensor = ZTensor::arr([
    [10, 20],
    [30, 40],
    [50, 60]
]);
print_r($arr_tensor->toArray());
```

### Deep Copy - `copy()`

```php
$original = ZTensor::arr([1, 2, 3]);
$copy = $original->copy();
print_r($original->toArray());
print_r($copy->toArray());
```

---

## Special Tensors

### Zeros Tensor - `zeros()`

```php
$zeros = ZTensor::zeros([2, 3]);
print_r($zeros->toArray());
// Output: [[0, 0, 0], [0, 0, 0]]
```

### Ones Tensor - `ones()`

```php
$ones = ZTensor::ones([3, 2]);
print_r($ones->toArray());
// Output: [[1, 1], [1, 1], [1, 1]]
```

### Constant Value Tensor - `full()`

```php
$full = ZTensor::full([2, 2], 7.5);
print_r($full->toArray());
// Output: [[7.5, 7.5], [7.5, 7.5]]
```

### Identity Matrix - `identity()`

```php
$identity = ZTensor::identity(3);
print_r($identity->toArray());
// Output: [[1, 0, 0], [0, 1, 0], [0, 0, 1]]
```

### Eye Matrix - `eye()`

```php
$eye = ZTensor::eye(3, 4, 1); // 3 rows, 4 columns, upper diagonal
print_r($eye->toArray());
// Output: [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]
```

---

## Sequence Generation

### Range Sequence - `arange()`

Simple range (0 to 4):
```php
$arange1 = ZTensor::arange(5);
print_r($arange1->toArray());
// Output: [0, 1, 2, 3, 4]
```

Range with start and stop:
```php
$arange2 = ZTensor::arange(2, 8);
print_r($arange2->toArray());
// Output: [2, 3, 4, 5, 6, 7]
```

Range with step:
```php
$arange3 = ZTensor::arange(0, 10, 2.5);
print_r($arange3->toArray());
// Output: [0, 2.5, 5.0, 7.5]
```

### Linear Space - `linspace()`

```php
$linspace = ZTensor::linspace(0, 1, 5);
print_r($linspace->toArray());
// Output: [0, 0.25, 0.5, 0.75, 1.0]
```

### Logarithmic Space - `logspace()`

```php
$logspace = ZTensor::logspace(1, 3, 3); // 10^1, 10^2, 10^3
print_r($logspace->toArray());
// Output: [10, 100, 1000]
```

---

## Random Number Generation

### Uniform Random - `random()`

```php
$random = ZTensor::random([2, 3], 0.0, 10.0);
echo "Random tensor shape: ";
print_r($random->shape());
// Generates random values between 0.0 and 10.0
```

### Normal Distribution - `randn()`

```php
$randn = ZTensor::randn([2, 2], 0.0, 1.0);
echo "Normal distribution tensor shape: ";
print_r($randn->shape());
// Generates normally distributed random values
```

---

## Basic Arithmetic Operations

### Addition - `add()`

```php
$a = ZTensor::arr([[1, 2], [3, 4]]);
$b = ZTensor::arr([[5, 6], [7, 8]]);
$sum = $a->add($b);
print_r($sum->toArray());
// Output: [[6, 8], [10, 12]]
```

### Subtraction - `sub()`

```php
$a = ZTensor::arr([[1, 2], [3, 4]]);
$b = ZTensor::arr([[5, 6], [7, 8]]);
$diff = $a->sub($b);
print_r($diff->toArray());
// Output: [[-4, -4], [-4, -4]]
```

### Element-wise Multiplication - `mul()`

```php
$a = ZTensor::arr([[1, 2], [3, 4]]);
$b = ZTensor::arr([[5, 6], [7, 8]]);
$product = $a->mul($b);
print_r($product->toArray());
// Output: [[5, 12], [21, 32]]
```

### Element-wise Division - `divide()`

```php
$a = ZTensor::arr([[10, 20], [30, 40]]);
$b = ZTensor::arr([[2, 4], [5, 8]]);
$division = $a->divide($b);
print_r($division->toArray());
// Output: [[5, 5], [6, 5]]
```

### Scalar Multiplication - `scalarMultiply()`

```php
$a = ZTensor::arr([[1, 2], [3, 4]]);
$scalar_mul = $a->scalarMultiply(2.5);
print_r($scalar_mul->toArray());
// Output: [[2.5, 5], [7.5, 10]]
```

### Scalar Division - `scalarDivide()`

```php
$a = ZTensor::arr([[4, 8], [12, 16]]);
$scalar_div = $a->scalarDivide(2.0);
print_r($scalar_div->toArray());
// Output: [[2, 4], [6, 8]]
```

### Power - `pow()`

```php
$a = ZTensor::arr([[2, 3], [4, 5]]);
$power = $a->pow(2);
print_r($power->toArray());
// Output: [[4, 9], [16, 25]]
```

---

## Linear Algebra

### Matrix Multiplication - `matmul()`

```php
$matrix_a = ZTensor::arr([
    [1, 2, 3],
    [4, 5, 6]
]);
$matrix_b = ZTensor::arr([
    [7, 8],
    [9, 10],
    [11, 12]
]);
$matrix_product = $matrix_a->matmul($matrix_b);
print_r($matrix_product->toArray());
// Output: [[58, 64], [139, 154]]
```

### Transpose - `transpose()`

```php
$matrix = ZTensor::arr([
    [1, 2, 3],
    [4, 5, 6]
]);
$transposed = $matrix->transpose();
print_r($transposed->toArray());
// Output: [[1, 4], [2, 5], [3, 6]]
```

### Dot Product - `dot()`

Vector dot product:
```php
$vec1 = ZTensor::arr([1, 2, 3]);
$vec2 = ZTensor::arr([4, 5, 6]);
$dot_product = $vec1->dot($vec2);
echo "Dot product: $dot_product\n";
// Output: 32 (1*4 + 2*5 + 3*6)
```

Matrix-vector multiplication:
```php
$matrix = ZTensor::arr([[1, 2], [3, 4]]);
$vector = ZTensor::arr([5, 6]);
$result = $matrix->dot($vector);
print_r($result->toArray());
// Output: [17, 39]
```

---

## Mathematical Functions

### Absolute Value - `abs()`

```php
$negative = ZTensor::arr([-1, -2, 3, -4]);
$absolute = $negative->abs();
print_r($absolute->toArray());
// Output: [1, 2, 3, 4]
```

### Square Root - `sqrt()`

```php
$values = ZTensor::arr([1, 4, 9, 16]);
$sqrt_result = $values->sqrt();
print_r($sqrt_result->toArray());
// Output: [1, 2, 3, 4]
```

### Exponential - `exp()`

```php
$values = ZTensor::arr([0, 1, 2]);
$exp_result = $values->exp();
print_r($exp_result->toArray());
// Output: [1, 2.718..., 7.389...]
```

### Natural Logarithm - `log()`

```php
$values = ZTensor::arr([1, 2.718, 7.389]);
$log_result = $values->log();
print_r($log_result->toArray());
// Output: [0, 1, 2] (approximately)
```

---

## Activation Functions

### Sigmoid - `sigmoid()`

```php
$data = ZTensor::arr([-2, -1, 0, 1, 2]);
$sigmoid_result = $data->sigmoid();
print_r($sigmoid_result->toArray());
// Output: [0.119, 0.269, 0.5, 0.731, 0.881] (approximately)
```

### Sigmoid Derivative - `sigmoidDerivative()`

```php
$sigmoid_values = ZTensor::arr([0.119, 0.269, 0.5, 0.731, 0.881]);
$sigmoid_deriv = $sigmoid_values->sigmoidDerivative();
print_r($sigmoid_deriv->toArray());
// Output: derivative values
```

### ReLU - `relu()`

```php
$data = ZTensor::arr([-2, -1, 0, 1, 2]);
$relu_result = $data->relu();
print_r($relu_result->toArray());
// Output: [0, 0, 0, 1, 2]
```

### ReLU Derivative - `reluDerivative()`

```php
$data = ZTensor::arr([-2, -1, 0, 1, 2]);
$relu_deriv = $data->reluDerivative();
print_r($relu_deriv->toArray());
// Output: [0, 0, 0, 1, 1]
```

### Leaky ReLU - `leakyRelu()`

```php
$data = ZTensor::arr([-2, -1, 0, 1, 2]);
$leaky_relu = $data->leakyRelu(0.1);
print_r($leaky_relu->toArray());
// Output: [-0.2, -0.1, 0, 1, 2]
```

### Leaky ReLU Derivative - `leakyReluDerivative()`

```php
$data = ZTensor::arr([-2, -1, 0, 1, 2]);
$leaky_deriv = $data->leakyReluDerivative(0.1);
print_r($leaky_deriv->toArray());
// Output: [0.1, 0.1, 0.1, 1, 1]
```

### Hyperbolic Tangent - `tanh()`

```php
$data = ZTensor::arr([-2, -1, 0, 1, 2]);
$tanh_result = $data->tanh();
print_r($tanh_result->toArray());
// Output: [-0.964, -0.762, 0, 0.762, 0.964] (approximately)
```

### Tanh Derivative - `tanhDerivative()`

```php
$tanh_values = ZTensor::arr([-0.964, -0.762, 0, 0.762, 0.964]);
$tanh_deriv = $tanh_values->tanhDerivative();
print_r($tanh_deriv->toArray());
// Output: derivative values
```

### Softmax - `softmax()`

```php
$data = ZTensor::arr([
    [1, 2, 3],
    [4, 5, 6]
]);
$softmax_result = $data->softmax();
print_r($softmax_result->toArray());
// Output: normalized probabilities for each row
```

### Softmax Derivative - `softmaxDerivative()`

```php
$softmax_values = ZTensor::arr([
    [0.09, 0.244, 0.665],
    [0.09, 0.244, 0.665]
]);
$softmax_deriv = $softmax_values->softmaxDerivative();
print_r($softmax_deriv->toArray());
// Output: derivative values
```

---

## Statistics and Aggregations

### Total Sum - `sumtotal()`

```php
$tensor = ZTensor::arr([
    [1, 2, 3],
    [4, 5, 6]
]);
$total_sum = $tensor->sumtotal();
echo "Total sum: $total_sum\n";
// Output: 21
```

### Mean - `mean()`

```php
$tensor = ZTensor::arr([1, 2, 3, 4, 5]);
$mean_value = $tensor->mean();
echo "Mean: $mean_value\n";
// Output: 3.0
```

### Minimum - `min()`

```php
$tensor = ZTensor::arr([5, 2, 8, 1, 9]);
$min_value = $tensor->min();
echo "Minimum: $min_value\n";
// Output: 1
```

### Maximum - `max()`

```php
$tensor = ZTensor::arr([5, 2, 8, 1, 9]);
$max_value = $tensor->max();
echo "Maximum: $max_value\n";
// Output: 9
```

### Standard Deviation - `std()`

```php
$tensor = ZTensor::arr([1, 2, 3, 4, 5]);
$std_value = $tensor->std();
echo "Standard deviation: $std_value\n";
// Output: approximately 1.58
```

### Sum - `sum()` (v0.5.0+)

**UPDATED (v0.5.0):** Method refactored for NumPy/PyTorch compatibility!

#### Global Sum (all elements)
```php
$tensor = ZTensor::arr([[1, 2, 3], [4, 5, 6]]);
$total = $tensor->sum();
echo "Total: " . $total->toArray()[0] . "\n";  // 21 (as scalar tensor)
```

#### Sum Along Specific Axis
```php
$tensor = ZTensor::arr([
    [1, 2, 3],
    [4, 5, 6]
]);

// Sum along axis 0 (down columns)
$sum_axis_0 = $tensor->sum(0);
print_r($sum_axis_0->toArray());
// Output: [5, 7, 9]

// Sum along axis 1 (across rows)
$sum_axis_1 = $tensor->sum(1);
print_r($sum_axis_1->toArray());
// Output: [6, 15]
```

#### NumPy-style Negative Indexing
```php
$tensor = ZTensor::arr([
    [1, 2, 3],
    [4, 5, 6]
]);

// -1 = last axis (same as axis 1 for 2D)
$sum_last = $tensor->sum(-1);
print_r($sum_last->toArray());
// Output: [6, 15]
```

#### Error Handling
```php
$tensor = ZTensor::arr([[1, 2, 3], [4, 5, 6]]);

// Invalid axis (throws exception)
try {
    $tensor->sum(5);  // Out of bounds
} catch (Exception $e) {
    echo "Error: " . $e->getMessage() . "\n";
}

// Non-integer axis (throws TypeError)
try {
    $tensor->sum("0");  // Wrong type
} catch (TypeError $e) {
    echo "Type error: " . $e->getMessage() . "\n";
}
```

**API Changes:**
- ‚úÖ Now returns `ZTensor` in ALL cases (always consistent)
- ‚úÖ Removed mandatory `$other` parameter
- ‚úÖ Optional `$axis` parameter for reduction
- ‚úÖ NumPy-compatible syntax
- ‚úÖ Validates axis bounds serially before parallel execution
- ‚úÖ Exception-safe with proper error messages

---

## Comparison and Clipping

### Greater Than - `greater()`

```php
$a = ZTensor::arr([1, 5, 3, 8, 2]);
$b = ZTensor::arr([2, 4, 3, 6, 9]);
$greater_result = $a->greater($b);
print_r($greater_result->toArray());
// Output: [0, 1, 0, 1, 0]
```

### Clip Values - `clip()`

```php
$data = ZTensor::arr([[-2, 5, 10], [0, 15, 1]]);
$clipped = ZTensor::clip($data, 0, 10);
print_r($clipped->toArray());
// Output: [[0, 5, 10], [0, 10, 1]]
```

### Element-wise Minimum with Scalar - `minimum()`

```php
$data = ZTensor::arr([1, 5, 3, 8, 2]);
$min_result = ZTensor::minimum($data, 4.0);
print_r($min_result->toArray());
// Output: [1, 4, 3, 4, 2]
```

### Element-wise Maximum with Scalar - `maximum()`

```php
$data = ZTensor::arr([1, 5, 3, 8, 2]);
$max_result = ZTensor::maximum($data, 4.0);
print_r($max_result->toArray());
// Output: [4, 5, 4, 8, 4]
```

---

## Shape Manipulation

### Get Shape - `shape()`

```php
$tensor = ZTensor::arr([
    [1, 2, 3, 4],
    [5, 6, 7, 8]
]);
$shape = $tensor->shape();
print_r($shape);
// Output: [2, 4]
```

### Number of Dimensions - `ndim()`

```php
$tensor = ZTensor::arr([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]);
$dimensions = $tensor->ndim();
echo "Number of dimensions: $dimensions\n";
// Output: 3
```

### Total Size - `size()`

```php
$tensor = ZTensor::arr([[1, 2, 3], [4, 5, 6]]);
$total_size = $tensor->size();
echo "Total elements: $total_size\n";
// Output: 6
```

### Check if Empty - `isEmpty()`

```php
$tensor = ZTensor::arr([1, 2, 3]);
$is_empty = $tensor->isEmpty();
echo "Is empty: " . ($is_empty ? "yes" : "no") . "\n";
// Output: no
```

### Reshape - `reshape()`

```php
$tensor = ZTensor::arr([
    [1, 2, 3, 4],
    [5, 6, 7, 8]
]);
$reshaped = $tensor->reshape([4, 2]);
print_r($reshaped->toArray());
// Output: [[1, 2], [3, 4], [5, 6], [7, 8]]
```

### Convert to Array - `toArray()`

```php
$tensor = ZTensor::arr([[1, 2], [3, 4]]);
$php_array = $tensor->toArray();
print_r($php_array);
// Output: [[1, 2], [3, 4]]
```

### Access Element by Index - `key()`

```php
$tensor = ZTensor::arr([
    [1, 2, 3],
    [4, 5, 6]
]);
$element = $tensor->key([1, 2]);
echo "Element at [1,2]: $element\n";
// Output: 6
```

---

## Special Operations

### Broadcasting - `broadcast()`

```php
$matrix = ZTensor::arr([
    [1, 2],
    [3, 4],
    [5, 6]
]);
$bias = ZTensor::arr([10, 20]);
$broadcasted = $matrix->broadcast($bias);
print_r($broadcasted->toArray());
// Output: [[11, 22], [13, 24], [15, 26]]
```

### Tile/Repeat - `tile()`

```php
$tensor = ZTensor::arr([
    [1, 2],
    [3, 4]
]);
$tiled = ZTensor::tile($tensor, 3);
print_r($tiled->toArray());
// Output: [[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]]
```

---

## Autograd & Gradient Operations (Experimental)

> ‚ö†Ô∏è **Note:** Autograd infrastructure is experimental. Backward pass is not yet fully implemented.

### Enabling Gradient Tracking - `requiresGrad()`

```php
// Enable gradient tracking
$tensor = ZTensor::arr([1.0, 2.0, 3.0]);
$tensor->requiresGrad(true);

// Or method chaining
$t = ZTensor::arr([[1, 2], [3, 4]])->requiresGrad(true);
```

### Checking Gradient Status - `isRequiresGrad()`

```php
$tensor = ZTensor::arr([1, 2, 3]);
$tensor->requiresGrad(true);

if ($tensor->isRequiresGrad()) {
    echo "This tensor tracks gradients\n";
}
```

### Initializing Gradients - `ensureGrad()`

```php
$tensor = ZTensor::arr([1.0, 2.0, 3.0]);
$tensor->requiresGrad(true);

// Ensure gradient is allocated
$tensor->ensureGrad();

// Get gradient tensor
$grad = $tensor->getGrad();
if ($grad !== null) {
    echo "Gradient allocated: " . $grad->size() . " elements\n";
}
```

### Getting Current Gradient - `getGrad()`

```php
$tensor = ZTensor::arr([[1.0, 2.0], [3.0, 4.0]]);
$tensor->requiresGrad(true);
$tensor->ensureGrad();

$gradient = $tensor->getGrad();
if ($gradient !== null) {
    echo "Gradient shape: ";
    print_r($gradient->shape());
    echo "Gradient values: ";
    print_r($gradient->toArray());
}
```

### Zeroing Gradients - `zeroGrad()`

```php
$tensor = ZTensor::arr([1.0, 2.0, 3.0]);
$tensor->requiresGrad(true);
$tensor->ensureGrad();

// Accumulate gradients...
// Then zero them for next iteration
$tensor->zeroGrad();
```

### Automatic Differentiation - Autograd Methods

**‚ö†Ô∏è Experimental: These methods prepare infrastructure for backpropagation. Full backward pass not yet implemented.**

#### Adding with Autograd - `addAutograd()`
```php
$a = ZTensor::arr([1.0, 2.0])->requiresGrad(true);
$b = ZTensor::arr([3.0, 4.0])->requiresGrad(true);

$c = ZTensor::addAutograd($a, $b);
echo "Result: ";
print_r($c->toArray());
// Output: [4, 6]
```

#### Subtracting with Autograd - `subAutograd()`
```php
$a = ZTensor::arr([5.0, 7.0])->requiresGrad(true);
$b = ZTensor::arr([2.0, 3.0])->requiresGrad(true);

$c = ZTensor::subAutograd($a, $b);
echo "Result: ";
print_r($c->toArray());
// Output: [3, 4]
```

#### Multiplying with Autograd - `mulAutograd()`
```php
$a = ZTensor::arr([2.0, 3.0])->requiresGrad(true);
$b = ZTensor::arr([4.0, 5.0])->requiresGrad(true);

$c = ZTensor::mulAutograd($a, $b);
echo "Result: ";
print_r($c->toArray());
// Output: [8, 15]
```

#### Sum with Autograd - `sumAutograd()`
```php
$tensor = ZTensor::arr([[1.0, 2.0], [3.0, 4.0]])->requiresGrad(true);

$sum = ZTensor::sumAutograd($tensor);
echo "Sum result: " . $sum->toArray()[0] . "\n";
// Output: 10
```

### Backward Pass - `backward()`

**‚ö†Ô∏è Experimental: Currently infrastructure-only. Full backpropagation not yet fully implemented.**

```php
// Example structure (backward pass not yet complete)
$x = ZTensor::arr([1.0, 2.0, 3.0])->requiresGrad(true);
$y = ZTensor::addAutograd($x, $x);
$loss = ZTensor::sumAutograd($y);

// Would trigger backpropagation (when fully implemented)
// $loss->backward();

// For now, you can manually zero and accumulate gradients
// $loss->ensureGrad();
// $loss->zeroGrad();
```

### String Representation - `__toString()`

```php
$tensor = ZTensor::arr([1, 2, 3, 4, 5, 6]);
$tensor->reshape([2, 3]);

echo $tensor;
// Output: ZTensor([2, 3], dtype: float32, 6 elements)
```

---

## Special Operations

### Broadcasting - `broadcast()`

```php
$matrix = ZTensor::arr([
    [1, 2],
    [3, 4],
    [5, 6]
]);
$bias = ZTensor::arr([10, 20]);
$broadcasted = $matrix->broadcast($bias);
print_r($broadcasted->toArray());
// Output: [[11, 22], [13, 24], [15, 26]]
```

---

## üöÄ GPU Acelera√ß√£o Detalhada

### Transfer√™ncia de Dados - `toGpu()` e `toCpu()`

```php
$tensor = ZTensor::random([5000, 5000]);

// Move para GPU
$tensor->toGpu();

// Opera√ß√µes na GPU s√£o aceleradas
$result = $tensor->relu();
$result->add($other_tensor);

// Volta para CPU
$tensor->toCpu();
$php_array = $tensor->toArray();
```

### Verificar Localiza√ß√£o do Tensor - `isOnGpu()`

```php
$tensor = ZTensor::arr([[1, 2], [3, 4]]);

if ($tensor->isOnGpu()) {
    echo "Tensor est√° na GPU\n";
} else {
    echo "Tensor est√° na CPU\n";
}
```

### Liberar Mem√≥ria GPU - `freeDevice()`

```php
// Ap√≥s usar muitos tensores na GPU
$tensor1->freeDevice();
$tensor2->freeDevice();
$tensor3->freeDevice();

// Ou em um loop
foreach ($large_tensors as $tensor) {
    $tensor->toGpu();
    $tensor->relu();
    $tensor->toCpu();
    $tensor->freeDevice();  // Libera GPU imediatamente
}
```

### Exemplo Pr√°tico: ML com GPU

```php
use ZMatrix\ZTensor;

// Dados de treinamento grandes
$X_train = ZTensor::random([10000, 100]);
$y_train = ZTensor::random([10000, 10]);

// Move para GPU
$X_train->toGpu();
$y_train->toGpu();

// Forward pass
$hidden = $X_train->matmul($W1);
$hidden->add($b1);
$hidden->relu();

// Aplicar dropout, etc
$output = $hidden->matmul($W2);
$output->softmax();

// Volta para CPU para processing
$output->toCpu();
$predictions = $output->toArray();

// Libera mem√≥ria GPU
$X_train->freeDevice();
$y_train->freeDevice();
```

---

## M√©todos Adicionais

### Acessar Elemento por √çndice - `key()`

```php
$tensor = ZTensor::arr([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
]);

// Acessa elemento em posi√ß√£o [1, 2]
$element = $tensor->key([1, 2]);
echo "Element: $element\n";  // Output: 6

// Acessa elemento em 3D
$tensor_3d = ZTensor::arr([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]);
$elem = $tensor_3d->key([0, 1, 0]);
echo "Element: $elem\n";  // Output: 3
```

### Fun√ß√µes de Extremo - `minimum()` e `maximum()`

```php
// Element-wise minimum with scalar
$data = ZTensor::arr([1, 5, 3, 8, 2]);
$min_result = ZTensor::minimum($data, 4.0);
print_r($min_result->toArray());
// Output: [1, 4, 3, 4, 2]

// Element-wise maximum with scalar
$max_result = ZTensor::maximum($data, 4.0);
print_r($max_result->toArray());
// Output: [4, 5, 4, 8, 4]
```

---

---

## üìö Complete API Reference - Resumo de Todos os M√©todos

### Tabela R√°pida de Todos os 62 M√©todos

| Categoria | M√©todo | Tipo | Descri√ß√£o |
|-----------|--------|------|-----------|
| **Cria√ß√£o** | `__construct()` | Constructor | Cria tensor de array ou vazio |
| | `arr()` | Static | Factory method para criar de array |
| | `safe()` | Static | Cria√ß√£o segura com valida√ß√£o |
| | `copy()` | Instance | Deep copy do tensor |
| **Cria√ß√£o - Especiais** | `zeros()` | Static | Tensor com zeros |
| | `ones()` | Static | Tensor com uns |
| | `full()` | Static | Tensor preenchido com valor |
| | `identity()` | Static | Matriz identidade |
| | `eye()` | Static | Matriz diagonal |
| | `random()` | Static | Valores aleat√≥rios uniformes |
| | `randn()` | Static | Valores aleat√≥rios normais |
| **Sequ√™ncias** | `arange()` | Static | Sequ√™ncia com passo |
| | `linspace()` | Static | Espa√ßo linear |
| | `logspace()` | Static | Espa√ßo logar√≠tmico |
| **Aritm√©tica** | `add()` | Instance | Adi√ß√£o elemento a elemento |
| | `sub()` | Instance | Subtra√ß√£o elemento a elemento |
| | `mul()` | Instance | Multiplica√ß√£o elemento a elemento |
| | `divide()` | Instance | Divis√£o elemento a elemento |
| | `pow()` | Instance | Pot√™ncia elemento a elemento |
| | `scalarMultiply()` | Instance | Multiplica√ß√£o por escalar |
| | `scalarDivide()` | Instance | Divis√£o por escalar |
| **√Ålgebra Linear** | `matmul()` | Instance | Multiplica√ß√£o de matrizes |
| | `dot()` | Instance | Produto escalar/dot product |
| | `transpose()` | Instance | Transposi√ß√£o (2D) |
| **Fun√ß√µes Matem√°ticas** | `abs()` | Instance | Valor absoluto |
| | `sqrt()` | Instance | Raiz quadrada |
| | `exp()` | Instance | Exponencial |
| | `log()` | Instance | Logaritmo natural |
| **Ativa√ß√µes** | `sigmoid()` | Instance | Fun√ß√£o sigmoid |
| | `sigmoidDerivative()` | Instance | Derivada sigmoid |
| | `relu()` | Instance | ReLU |
| | `reluDerivative()` | Instance | Derivada ReLU |
| | `leakyRelu()` | Instance | Leaky ReLU com alpha |
| | `leakyReluDerivative()` | Instance | Derivada Leaky ReLU |
| | `tanh()` | Instance | Tangente hiperb√≥lica |
| | `tanhDerivative()` | Instance | Derivada tanh |
| | `softmax()` | Instance | Softmax |
| | `softmaxDerivative()` | Instance | Derivada softmax |
| **Estat√≠sticas** | `sum()` | Instance | Soma global ou por eixo (v0.5.0+) |
| | `sumtotal()` | Instance | Soma total de elementos |
| | `mean()` | Instance | M√©dia |
| | `min()` | Instance | M√≠nimo |
| | `max()` | Instance | M√°ximo |
| | `std()` | Instance | Desvio padr√£o |
| **Compara√ß√£o** | `greater()` | Instance | Compara√ß√£o > |
| | `clip()` | Static | Limita valores min-max |
| | `minimum()` | Static | Min elemento com escalar |
| | `maximum()` | Static | Max elemento com escalar |
| **Shape & Info** | `shape()` | Instance | Retorna shape |
| | `ndim()` | Instance | N√∫mero de dimens√µes |
| | `size()` | Instance | Total de elementos |
| | `isEmpty()` | Instance | Verifica se vazio |
| | `reshape()` | Instance | Muda shape |
| | `toArray()` | Instance | Converte para array PHP |
| **Acesso** | `key()` | Instance | Acessa elemento por √≠ndices |
| **Manipula√ß√£o** | `broadcast()` | Instance | Broadcast com bias |
| | `tile()` | Static | Repete tensor N vezes |
| **Autograd** | `requiresGrad()` | Instance | Ativa rastreamento gradiente |
| | `isRequiresGrad()` | Instance | Verifica rastreamento |
| | `ensureGrad()` | Instance | Aloca tensor de gradiente |
| | `zeroGrad()` | Instance | Zera gradiente acumulado |
| | `getGrad()` | Instance | Obt√©m tensor de gradiente |
| | `addAutograd()` | Static | Adi√ß√£o com autograd (exp) |
| | `subAutograd()` | Static | Subtra√ß√£o com autograd (exp) |
| | `mulAutograd()` | Static | Multiplica√ß√£o com autograd (exp) |
| | `sumAutograd()` | Static | Soma com autograd (exp) |
| **GPU** | `toGpu()` | Instance | Move para GPU |
| | `toCpu()` | Instance | Move para CPU |
| | `isOnGpu()` | Instance | Verifica se est√° em GPU |
| | `freeDevice()` | Instance | Libera mem√≥ria GPU |
| **String** | `__toString()` | Instance | Representa√ß√£o em string |

### Categorias de Uso

**Para iniciantes:**
- `arr()` - criar tensores
- `shape()`, `toArray()` - inspecionar
- `add()`, `sub()`, `mul()` - aritm√©tica b√°sica
- `reshape()`, `transpose()` - manipula√ß√£o de shape

**Para machine learning:**
- Todas as fun√ß√µes de ativa√ß√£o (`relu`, `sigmoid`, `softmax`)
- `matmul()` para redes neurais
- `requiresGrad()` para preparar autograd
- `toGpu()`, `toCpu()` para acelera√ß√£o

**Para computa√ß√£o num√©rica:**
- `random()`, `randn()` para inicializa√ß√£o
- Fun√ß√µes matem√°ticas: `exp()`, `log()`, `sqrt()`, `pow()`
- Estat√≠sticas: `mean()`, `std()`, `sum()`
- Compara√ß√£o: `greater()`, `clip()`, `minimum()`, `maximum()`

**Para processamento em lote:**
- `broadcast()` para aplicar bias
- `tile()` para repetir opera√ß√µes
- `dot()` para agrega√ß√£o
- GPU methods para grandes volumes

---

## üîß Troubleshooting

### Problema: Erro de compila√ß√£o "cuda.h not found"

**Solu√ß√£o:**
```bash
# Especifique o caminho do CUDA durante configure
./configure --enable-zmatrix --with-cuda-path=/usr/local/cuda

# Ou verifique se CUDA est√° instalado
nvcc --version
```

### Problema: "libcuda.so not found" em runtime

**Solu√ß√£o:**
```bash
# O sistema tenta encontrar libcuda.so automaticamente
# Se n√£o encontrar, adicione ao LD_LIBRARY_PATH
export LD_LIBRARY_PATH=/usr/lib/wsl/lib:$LD_LIBRARY_PATH  # WSL
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH  # CUDA Toolkit
```

### Problema: GPU methods n√£o funcionam (Exception: CUDA support not available)

**Causa:** CUDA n√£o foi encontrado durante compila√ß√£o ou em runtime

**Solu√ß√£o:**
- GPU √© opcional! A biblioteca continua funcionando com CPU
- Use opera√ß√µes CPU normalmente:
```php
$tensor = ZTensor::arr([[1, 2], [3, 4]]);
$tensor->add([1, 1]);  // Funciona em CPU
```

- Se precisa de GPU, instale drivers NVIDIA:
```bash
sudo apt-get install nvidia-driver-XXX  # Verifique vers√£o recomendada
```

### Problema: WSL2 n√£o detecta GPU

**Solu√ß√£o:**
```bash
# Verifique se drivers NVIDIA est√£o instalados (no Windows)
nvidia-smi  # No PowerShell

# Em WSL, configure para usar GPU:
./configure --enable-zmatrix
# Sistema detecta automaticamente WSL e configura caminhos corretos
```

### Problema: Compila√ß√£o lenta ou falha com "make -j"

**Solu√ß√£o:**
```bash
# Use menos threads de compila√ß√£o
make -j2  # Ao inv√©s de -j$(nproc)

# Ou limite mem√≥ria
make -j$(( $(nproc) / 2 ))
```

### Problema: "PHP Fatal error: Class 'ZMatrix\ZTensor' not found"

**Solu√ß√£o:**
```bash
# Verifique se extens√£o est√° carregada
php -m | grep zmatrix

# Se n√£o aparecer, adicione ao php.ini
echo "extension=zmatrix.so" | sudo tee -a /etc/php/8.x/cli/php.ini

# Recarregue
php -r "echo 'OK';"
```

### Problema: Performance ruim em GPU

**Causas:**
- Tensor muito pequeno (< 200k elementos)
- Overhead de H2D/D2H transfer maior que ganho de c√°lculo
- GPU ocupada por outro processo

**Solu√ß√£o:**
```php
// Use GPU apenas para opera√ß√µes grandes
if ($tensor->size() > 200000) {
    $tensor->toGpu();
    $tensor->relu();
    $tensor->toCpu();
} else {
    // CPU √© mais r√°pido para tensores pequenos
    $tensor->relu();
}
```

### Problema: Out of GPU Memory

**Solu√ß√£o:**
```php
// Libere mem√≥ria explicitamente
$tensor->freeDevice();

// Ou avoid keeping many large tensors on GPU
foreach ($tensors as $t) {
    $t->toGpu();
    // process...
    $t->toCpu();
    $t->freeDevice();  // Free GPU memory
}
```

### Problema: Resultados diferentes entre CPU e GPU

**Causa Comum:** Ordem de opera√ß√µes, tipo de dado, ou precis√£o num√©rica

**Solu√ß√£o:**
```php
// Garantir mesmos dados
$a_cpu = $a->copy();  // C√≥pia para CPU
$a_gpu = $a->copy();  // C√≥pia para GPU
$a_gpu->toGpu();

// Opera√ß√µes id√™nticas
$a_cpu->add($b);
$a_gpu->add($b);

// Comparar (com toler√¢ncia num√©rica)
$diff = abs($a_cpu->sum() - $a_gpu->sum());
assert($diff < 1e-5);  // Deve ser pr√≥ximo
```

---

## Support

For issues related to the ZTensor extension itself, please refer to the official repository or documentation provided by the extension maintainers.
## üôå Contribution

Contributions are welcome! Feel free to open issues or submit pull requests.

## üìÑ License

MIT License. See `LICENSE`.
